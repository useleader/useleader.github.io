<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>WarmFire</title>
    <link>http://www.warmfire.com/</link>
    
    <atom:link href="http://www.warmfire.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>不忘初心，方得始终</description>
    <pubDate>Wed, 06 Dec 2023 11:22:37 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>数分第14周学习日志</title>
      <link>http://www.warmfire.com/2023/12/06/%E6%95%B0%E5%88%86%E7%AC%AC14%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</link>
      <guid>http://www.warmfire.com/2023/12/06/%E6%95%B0%E5%88%86%E7%AC%AC14%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</guid>
      <pubDate>Wed, 06 Dec 2023 11:16:29 GMT</pubDate>
      
      <description>&lt;p&gt;0x00 前言&lt;/p&gt;
&lt;p&gt;定积分&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>0x00 前言</p><p>定积分</p><span id="more"></span><h2><span id="1-定积分的定义与性质">1 定积分的定义与性质</span></h2><h3><span id="1-定积分概念的物理背景">1 定积分概念的物理背景</span></h3><p>求曲线围成的面积：</p><ul><li>区间分割</li><li>求累加和</li><li>取极限</li></ul><h3><span id="2-定积分的数学定义">2 定积分的数学定义</span></h3><ul><li>设 $f(x)$在[a,b]上有定义，存在实数I，对于 $\forall \epsilon&gt;0,\exists \delta&gt;0,\forall$分割 $\pi:x_0=a&lt;x_1&lt;x_2&lt;\cdots&lt;x_n=b,$当分割的细度 $||\pi||=\max_{1\leq i\leq n}(\Delta x_i)&lt;\delta, \Delta x_i=x_i-x_{i-1}$时，对任意 $\xi_i\in[x_{i-1},x_i], |\sum^n_{i=1}f(\xi_i)(x_i-x_{i-1})-I|&lt;\epsilon$，称 $f(x)$在[a,b]上黎曼可积，I为  $f(x)$在[a,b]上的定积分或黎曼积分<ul><li>记为 $\int^b_a f(x)dx=I$</li><li>注1：$\xi_i$是 $[x_{i-1},x_i]$中的一点</li><li>注2：积分的定义用 $\epsilon-\delta$语言来描述，极限符号表示为 $\lim_{||\pi||\rightarrow0}\sum^n_{i=1}f(\xi_i)(x_i-x_{i-1})=I$，与普通的函数极限不同</li></ul></li><li>$\int^b_a f(x)dx=I=\lim_{||\pi||\rightarrow0}\sum^n_{i=1}f(\xi_i)(x_i-x_{i-1}),\xi_i\in[x_{i-1},x_i]$<ul><li>$\sum^n_{i=1}f(\xi_i)\Delta x_i$：黎曼和</li><li>$f(x)$：被积函数</li><li>$x$：积分变量</li><li>$[a,b]$：积分区间</li><li>注3：$\int^b_a f(x)dx=\int^b_a f(t)dt=\int^b_a f(r)dr$与自变量选取无关</li><li>注4：若 $a&gt;b$，$\int^b_a f(x)dx=-\int^a_b f(x)dx$</li></ul></li></ul><h3><span id="3-定积分的基本性质">3 定积分的基本性质</span></h3><ul><li>假设 $f(x),g(x)$在 $[a,b]$上可积</li></ul><h4><span id="线性性质">线性性质</span></h4><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203151433730.png" alt="image-20231203151433730"></li></ul><h4><span id="保序性">保序性</span></h4><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203151507319.png" alt="image-20231203151507319"></li></ul><h4><span id="积分中值定理">积分中值定理</span></h4><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203151807426.png" alt="image-20231203151807426"></li></ul><h4><span id="两个不等式">两个不等式</span></h4><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203153440271.png" alt="image-20231203153440271"><ul><li>构造 $\int_a^b(f(x)+tg(x))^2dx$，之后看作关于t的一元二次方程</li></ul></li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203153735219.png" alt="image-20231203153735219"></li></ul><h2><span id="2-定积分的存在定理">2 定积分的存在定理</span></h2><h3><span id="函数可积性理论">函数可积性理论</span></h3><h4><span id="定理1-可积的必要条件">定理1 可积的必要条件</span></h4><ul><li>函数f在[a,b]上可积，则f在[a,b]上有界<ul><li>注：可积必有界，有界未必可积</li><li>迪利克莱函数：达布上和与达布下和不相同，有理数的稠密性导致取得两种极限</li></ul></li></ul><h4><span id="定义1-达布上和与达布下和">定义1 达布上和与达布下和</span></h4><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203160209656.png" alt="image-20231203160209656"></li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203160356352.png" alt="image-20231203160356352"></li></ul><h4><span id="定理-2">定理 2</span></h4><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203160443260.png" alt="image-20231203160443260"></li><li>加密分割使得区间变得更小了，其中一个区间最值会有变化，造成了上和减，下和增</li><li>两种分割有联系</li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203161819406.png" alt="image-20231203161819406"></li><li>两种分割虽然没有联系，但是可以将两种分割合并构造第三种分割 $\pi^*$，之后与这个分割相比较来确定不等式</li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203161923254.png" alt="image-20231203161923254"></li><li>注：$f(x)$在 $[a,b]$上下确界分别为 $M,m$，则(1),(2)集合有界</li></ul><h4><span id="定理-3-达布定理">定理 3 达布定理</span></h4><ul><li>对于 $f(x)$在 $[a,b]$有界函数，则有 $\lim_{||\pi||\rightarrow0}\bar{S}(\pi,f)=\bar{I},\lim_{||\pi||\rightarrow0}\underline{S}(\pi,f)=\underline{I}$</li></ul><h4><span id="定理4">定理4</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203162736468.png" alt="image-20231203162736468"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203165049694.png" alt="image-20231203165049694"></p><h4><span id="定理-绝对可积">定理 绝对可积</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203165419715.png" alt="image-20231203165419715"></p><ul><li>定理逆命题不成立，比如说原本正好不同取值时取得是相反数，一作绝对值之后能相等了，但是他本身是不可积的，没错，我说的就是迪利克莱函数的姊妹版</li></ul><h4><span id="定理-积分对区间的可加性">定理 积分对区间的可加性</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203182435505.png" alt="image-20231203182435505"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203183340674.png" alt="image-20231203183340674"></p><h3><span id="可积函数类">可积函数类</span></h3><h4><span id="定理-1">定理 1</span></h4><p>设 $f(x)$在[a,b]上有界单调函数，则 $f(x)$在[a,b]可积</p><h4><span id="定理-2">定理 2</span></h4><p>设 $f(x)$在[a,b]上连续，则 $f(x)$在[a,b]可积</p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203201255566.png" alt="image-20231203201255566"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203211226217.png" alt="image-20231203211226217"></p><h4><span id="定理-3">定理 3</span></h4><p>$f$在[a,b]上有界，则可积的充要条件是：任意 $\epsilon &gt; 0, \eta&gt;0,$总存在分割T，使得属于T的所有小区间中，对于振幅 $\omega_{k’}\geq \epsilon$的对应的分割区间长度综合 $\sum_{k’}\Delta x_{k’}&lt;\eta$</p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203211820859.png" alt="image-20231203211820859"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203212033660.png" alt="image-20231203212033660"></p><h4><span id="例题">例题</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203212236347.png" alt="image-20231203212236347"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203212247930.png" alt="image-20231203212247930"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203212904636.png" alt="image-20231203212904636"></p><p>有理点有限个的原因是 $q\leq \frac{1}{\epsilon}$，而不是趋于无穷</p><p>为什么是2K个，我估计是 $\frac{p}{q},\frac{q-p}{q}$两个对称上了</p><p>先判断可积，根据下积分定积分。</p><h2><span id="3-微积分的基本定理">3 微积分的基本定理</span></h2><h3><span id="1-牛顿-莱布尼兹公式">1 牛顿-莱布尼兹公式</span></h3><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203215308742.png" alt="image-20231203215308742"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203215323358.png" alt="image-20231203215323358"></li></ul><h3><span id="例题">例题</span></h3><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203215553771.png" alt="image-20231203215553771"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203215604794.png" alt="image-20231203215604794"></li><li>分区间讨论不错</li></ul><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203215614812.png" alt="image-20231203215614812"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203215627646.png" alt="image-20231203215627646"></li><li>要认出来黎曼积分的等价形式</li></ul><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203215642375.png" alt="image-20231203215642375"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203215659194.png" alt="image-20231203215659194"></li><li>同样，结合之前学的求极限方法来求解</li></ul><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203215712930.png" alt="image-20231203215712930"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203215720592.png" alt="image-20231203215720592"></li><li>分区间讨论</li></ul><h3><span id="2-微积分基本定理变上限函数的连续与可导">2 微积分基本定理：变上限函数的连续与可导</span></h3><h4><span id="变上限积分函数">变上限积分函数</span></h4><p>$f\in R<a href="[a,b]上可积函数的集合">a,b</a>,\forall x\in[a,b],F(x)=\int_a^x f(t)dt$</p><ul><li>$F(x)$是变上限积分函数</li></ul><h4><span id="定理1">定理1</span></h4><p>若 $f\in R[a,b]$，则 $F(x)\in C[a,b]$</p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203220547031.png" alt="image-20231203220547031"></li></ul><h4><span id="定理-2-变上限积分函数的可导性质">定理 2 变上限积分函数的可导性质</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203220631098.png" alt="image-20231203220631098"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203220639460.png" alt="image-20231203220639460"></li><li>变上限积分函数不一定始终可导哈，需要<strong>连续</strong></li></ul><h4><span id="推论-1">推论 1</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203220708659.png" alt="image-20231203220708659"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203220721195.png" alt="image-20231203220721195"></li></ul><h4><span id="推论-2">推论 2</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203220732686.png" alt="image-20231203220732686"></p><ul><li>注意这个负号</li></ul><h4><span id="推论-3">推论 3</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203220749014.png" alt="image-20231203220749014"></p><h4><span id="定理-3-原函数存在定理">定理 3 原函数存在定理</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231203220757560.png" alt="image-20231203220757560"></p><h4><span id="例1">例1</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204082700058.png" alt="image-20231204082700058"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204082814246.png" alt="image-20231204082814246"></li></ul><h4><span id="例2">例2</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204082830558.png" alt="image-20231204082830558"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204082844003.png" alt="image-20231204082844003"></li></ul><h3><span id="典型例题">典型例题</span></h3><h4><span id="例1">例1</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204083835644.png" alt="image-20231204083835644"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204084304157.png" alt="image-20231204084304157"></li></ul><h4><span id="例2">例2</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204084758835.png" alt="image-20231204084758835"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204084812246.png" alt="image-20231204084812246"></li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204085939327.png" alt="image-20231204085939327"></li></ul><h4><span id="例3">例3</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204093515328.png" alt="image-20231204093515328"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204093527365.png" alt="image-20231204093527365"></li></ul><h2><span id="4-定积分的计算">4 定积分的计算</span></h2><h3><span id="分部积分公式与应用">分部积分公式与应用</span></h3><h4><span id="分部积分公式">分部积分公式</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204094245599.png" alt="image-20231204094245599"></p><hr><h4><span id="例1">例1</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204094318949.png" alt="image-20231204094318949"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204094328496.png" alt="image-20231204094328496"></li><li>先当不定积分算，算出来之后用上界下界求值</li></ul><h4><span id="例2">例2</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204095034904.png" alt="image-20231204095034904"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204095026618.png" alt="image-20231204095026618"></li><li>常规题型</li></ul><h4><span id="例3">例3</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204095138853.png" alt="image-20231204095138853"></p><ul><li>也是常规题型</li></ul><h4><span id="例4">例4</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204095249739.png" alt="image-20231204095249739"></p><ul><li>利用分部积分公式找递推关系式</li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204095445094.png" alt="image-20231204095445094"></li></ul><hr><h4><span id="定理-taylor公式的积分余项">定理 Taylor公式的积分余项</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204095612036.png" alt="image-20231204095612036"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204095747441.png" alt="image-20231204095747441"></li></ul><hr><h4><span id="例1">例1</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204100214914.png" alt="image-20231204100214914"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204101139982.png" alt="image-20231204101139982"></li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204101151175.png" alt="image-20231204101151175"></li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204101204795.png" alt="image-20231204101204795"></li><li>感觉是道直觉题，能够找到正确的路子就会好算很多</li></ul><h4><span id="例3">例3</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204101305527.png" alt="image-20231204101305527"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204102112360.png" alt="image-20231204102112360"></li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204102212951.png" alt="image-20231204102212951"></li></ul><hr><h3><span id="定积分的换元公式与应用">定积分的换元公式与应用</span></h3><h4><span id="定积分换元定理">定积分换元定理</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204102711705.png" alt="image-20231204102711705"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204103044155.png" alt="image-20231204103044155"></li><li>这里可能出现 $\varphi(t)$不单调进而导致上下界一样的情况吧。-&gt; 注意条件3，需要满足代换后的上下界满足 $\phi(\alpha)=a,\phi(\beta)=b$才可以</li></ul><h4><span id="推论1">推论1</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204103807654.png" alt="image-20231204103807654"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204103959553.png" alt="image-20231204103959553"></li></ul><h4><span id="推论2">推论2</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204104037798.png" alt="image-20231204104037798"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204104049428.png" alt="image-20231204104049428"></li><li>在计算中可以利用奇偶性与周期性简化运算</li></ul><hr><h4><span id="例2">例2</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204104233319.png" alt="image-20231204104233319"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204104240207.png" alt="image-20231204104240207"></li><li>这个正好约掉了，好巧妙</li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204105708753.png" alt="image-20231204105708753"></li></ul><h4><span id="例3">例3</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204105720358.png" alt="image-20231204105720358"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204105728467.png" alt="image-20231204105728467"></li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204105739366.png" alt="image-20231204105739366"></li></ul><h4><span id="例4">例4</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204105933170.png" alt="image-20231204105933170"></p><ul><li>常见错误，代换时忽略了换元的取值限制</li></ul><h4><span id="定积分换元定理2">定积分换元定理2</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204110152145.png" alt="image-20231204110152145"></p><hr><h4><span id="例1">例1</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204111818944.png" alt="image-20231204111818944"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204111834115.png" alt="image-20231204111834115"></li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204123446921.png" alt="image-20231204123446921"></li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204123459351.png" alt="image-20231204123459351"></li></ul><h4><span id="例2">例2</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204123514043.png" alt="image-20231204123514043"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204123523876.png" alt="image-20231204123523876"></li></ul><h4><span id="例3">例3</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204123538142.png" alt="image-20231204123538142"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204123549317.png" alt="image-20231204123549317"></li><li>这个变化抽象的很啊，要分清其中的常数部分</li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204123556166.png" alt="image-20231204123556166"></li></ul><h2><span id="5-定积分中值定理">5 定积分中值定理</span></h2><h3><span id="定积分第一中值定理">定积分第一中值定理</span></h3><h4><span id="定理-积分第一中值定理">定理 积分第一中值定理</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204130131424.png" alt="image-20231204130131424"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204130151753.png" alt="image-20231204130151753"></li><li>注意不变号的条件哦</li></ul><h4><span id="例1">例1</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204130219095.png" alt="image-20231204130219095"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204130228847.png" alt="image-20231204130228847"></li><li>这个分的有点意思</li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204162941659.png" alt="image-20231204162941659"></li></ul><h4><span id="例2">例2</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204162956564.png" alt="image-20231204162956564"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204170329789.png" alt="image-20231204170329789"></li></ul><h4><span id="例3">例3</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204170343514.png" alt="image-20231204170343514"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204170355773.png" alt="image-20231204170355773"></li><li>根据分母来讨论</li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204170402913.png" alt="image-20231204170402913"></li></ul><h3><span id="定积分第二中值定理">定积分第二中值定理</span></h3><h4><span id="积分第二中值定理">积分第二中值定理</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204190714477.png" alt="image-20231204190714477"></p><ul><li><p>非负且单调</p></li><li><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204190730270.png" alt="image-20231204190730270"></p></li><li><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204192128366.png" alt="image-20231204192128366"></p></li><li><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204192403599.png" alt="image-20231204192403599"></p></li><li><p>牛逼</p></li></ul><h4><span id="例1">例1</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204192420555.png" alt="image-20231204192420555"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204192430866.png" alt="image-20231204192430866"></li><li>确实是个好思路，通过中值定理来讲不好处理的量提出来</li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204192446467.png" alt="image-20231204192446467"></li></ul><h4><span id="例2">例2</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204192502378.png" alt="image-20231204192502378"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204192510804.png" alt="image-20231204192510804"></li></ul><h3><span id="定积分第三中值定理">定积分第三中值定理</span></h3><h4><span id="定理3-积分第三中值定理">定理3 积分第三中值定理</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204201226437.png" alt="image-20231204201226437"></p><ul><li>首先得可积，提出来的那个需要单调</li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204201249156.png" alt="image-20231204201249156"></li><li>重点得想出来构造这样一个函数</li></ul><h4><span id="例1">例1</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204201305984.png" alt="image-20231204201305984"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204201315142.png" alt="image-20231204201315142"></li></ul><h4><span id="例2">例2</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204201331622.png" alt="image-20231204201331622"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204201343110.png" alt="image-20231204201343110"></li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204201352063.png" alt="image-20231204201352063"></li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204201403660.png" alt="image-20231204201403660"></li></ul><h2><span id="6-勒贝格定理">6 勒贝格定理</span></h2><h3><span id="勒贝格定理">勒贝格定理</span></h3><h4><span id="定义-零测集">定义 零测集</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215402036.png" alt="image-20231204215402036"></p><h4><span id="例题">例题</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215420891.png" alt="image-20231204215420891"></p><ul><li>$|I_i|$ 应该是集合的长度和</li></ul><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215428030.png" alt="image-20231204215428030"></p><h4><span id="零测集性质">零测集性质</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215504336.png" alt="image-20231204215504336"></p><h4><span id="勒贝格定理">勒贝格定理</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215516864.png" alt="image-20231204215516864"></p><h4><span id="推论-函数可积的四则运算法则">推论 函数可积的四则运算法则</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215532614.png" alt="image-20231204215532614"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215652798.png" alt="image-20231204215652798"></li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215704401.png" alt="image-20231204215704401"></li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215714643.png" alt="image-20231204215714643"></li></ul><h3><span id="勒贝格定理应用">勒贝格定理应用</span></h3><h4><span id="例1">例1</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215751992.png" alt="image-20231204215751992"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215807584.png" alt="image-20231204215807584"></li><li>这个 $2k+\frac{1}{2}$应该是随便取得一个例子</li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215820837.png" alt="image-20231204215820837"></li><li>运用勒贝格定理的方法也就是先去找不连续点，找完观察集合是否为零测集</li></ul><h4><span id="例2">例2</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215833674.png" alt="image-20231204215833674"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215848509.png" alt="image-20231204215848509"></li></ul><h4><span id="例3">例3</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215906495.png" alt="image-20231204215906495"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215913436.png" alt="image-20231204215913436"></li></ul><h4><span id="例4">例4</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215926384.png" alt="image-20231204215926384"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215934788.png" alt="image-20231204215934788"></li></ul><h4><span id="例5">例5</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215947376.png" alt="image-20231204215947376"></p><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231204215954865.png" alt="image-20231204215954865"></li></ul><h2><span id="7-积分应用函数的磨光">7 积分应用：函数的磨光</span></h2><p><hr><br>版权信息</p>]]></content:encoded>
      
      
      
      
      <comments>http://www.warmfire.com/2023/12/06/%E6%95%B0%E5%88%86%E7%AC%AC14%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>GANLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator</title>
      <link>http://www.warmfire.com/2023/11/27/GANLM-Encoder-Decoder-Pre-training-with-an-Auxiliary-Discriminator/</link>
      <guid>http://www.warmfire.com/2023/11/27/GANLM-Encoder-Decoder-Pre-training-with-an-Auxiliary-Discriminator/</guid>
      <pubDate>Mon, 27 Nov 2023 14:58:34 GMT</pubDate>
      
      <description>&lt;p&gt;0x00 前言&lt;/p&gt;
&lt;p&gt;GANLM论文翻译&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>0x00 前言</p><p>GANLM论文翻译</p><span id="more"></span><h2><span id="abstract">Abstract</span></h2><p>Pre-trained models have achieved remarkable success in natural language processing (NLP). However, existing pre-training methods underutilize the benefits of language understanding for generation. Inspired by the idea of Generative Adversarial Networks (GANs), we propose a GAN-style model for encoder-decoder pretraining by introducing an auxiliary discriminator, unifying the ability of language under standing and generation in a single model. Our model, named as GANLM, is trained with two pre-training objectives: <strong>replaced token detection and replaced token denoising</strong>. Specifically, <strong>given masked source sentences, the generator outputs the target distribution and the discriminator predicts whether the target sampled tokens from distribution are incorrect. The target sentence is replaced with misclassified tokens to construct noisy previous context, which is used to generate the gold sentence</strong>. In general, both tasks improve the ability of language understanding and generation by selectively using the denoising data. Extensive experiments in language generation benchmarks show that GANLM with the powerful language understanding capability outperforms various strong pre-trained language models (PLMs) and achieves state-of-the-art performance.</p><blockquote><p>underutilize vt.未充分使用</p><p>auxiliary adj. 辅助的</p><p>unify v.联合</p><p>replace token detection 替换token检测</p><p>replace token denoising 替换token去噪</p><p>gold sentense</p></blockquote><p>预训练模型在自然语言处理(NLP)领域取得了显著的成功。然而，现有的预训练方法没有充分利用语言理解对生成的好处。受生成对抗网络(GANs)思想的启发，通过引入辅助判别器，提出了一种用于编码器-解码器预训练的gan风格的模型，将语言理解和生成的能力统一在单个模型中。该模型名为GANLM，使用两个预训练目标进行训练:<strong>替换token检测和替换token去噪</strong>。具体来说，<strong>给定掩码源句子，生成器输出目标分布，鉴别器预测来自分布的目标采样token是否不正确。将目标句子替换为错误分类的token，构建有噪声的前一个上下文，用于生成黄金句子</strong>。总的来说，这两项任务都通过有选择地使用去噪数据来提高语言理解和生成的能力。在语言生成基准上的广泛实验表明，具有强大语言理解能力的GANLM优于各种强大的预训练语言模型(plm)，并取得了最先进的性能。</p><h2><span id="1-introduction">1 Introduction</span></h2><p>​    The pre-training-then-fine-tuning paradigm has been proven successful in many natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Schick and Schütze, 2021). While there are various pre-training approaches for the encoder only architectures (Clark et al., 2020; Conneauet al., 2020), the encoder-decoder pre-training is underexplored, which is essential for natural language generation. To pre-train the entire encoder-decoder model, BART (Lewis et al., 2020) proposes a denoising language model objective and T5 (Raffelet al., 2020) pre-trains the models with a span corruption objective. Furthermore, mBART (Liu et al., \2020) and mT5 (Xue et al., 2021) extend them to be multilingual pre-trained language models. </p><p>​    预训练-然后微调的范式已被证明在许多自然语言处理任务中是成功的(Devlin等人，2019;Liu等人，2019;Schick and Schütze, 2021)。虽然有各种针对仅编码器架构的预训练方法(Clark等人，2020;Conneauet al.， 2020)，编码器-解码器预训练尚未得到充分开发，而这对自然语言生成至关重要。为了预训练整个编码器-解码器模型，BART (Lewis et al.， 2020)提出了去噪语言模型目标，T5 (Raffelet al.， 2020)预训练具有跨度破坏目标的模型。此外，mBART (Liu et al.， \2020)和mT5 (Xue et al.， 2021)将它们扩展为多语言预训练语言模型。</p><p>​    Unlike most encoder-decoder pre-training methods that simply apply sequence-to-sequence tasks on a single encoder-decoder architecture, we explore the approaches to pre-train the model in a GAN-style manner with an auxiliary discriminator. GAN (Goodfellow et al., 2014) performs well on both text and image generation tasks by combining the generator and discriminator. It aims to improve the ability of the generator to produce high quality samples, which is important for the encoder decoder pre-training when transferred to down stream generation tasks. Similarly, MaskGAN (Fedus et al., 2018) shows the GAN-like training can improve the quality of the autoregressive language model. Therefore, it is intuitive to leverage GAN to empower the encoder-decoder pre-training by unifying language understanding and generation. </p><p>​    与大多数仅在单个编码器-解码器架构上应用序列到序列任务的编码器-解码器预训练方法不同，本文探索了用辅助判别器以gan风格的方式预训练模型的方法。GAN (Goodfellow等人，2014)通过结合生成器和鉴别器，在文本和图像生成任务上表现良好。它旨在提高生成器产生高质量样本的能力，这对于将编码器-解码器预训练迁移到下游生成任务时很重要。同样，MaskGAN (Fedus et al.， 2018)表明类gan训练可以提高自回归语言模型的质量。因此，利用GAN通过统一语言理解和生成来增强编码器-解码器预训练是很直观的。</p><p>​    In this work, we propose a pre-training frame work GANLM, using GAN-style learning to improve the transferability of pre-trained language models for the natural language generation. Specifically, the encoder reads the masked source sentence and the generator obtains target distribution. Then, the discriminator distinguishes whether each token sampled from the target distribution matches the target gold sentence (replaced token detection). The misclassified tokens by discriminator are regarded as hard tokens for the generator to predict accurately. We replace original tokens in the target sentence with misclassified sampled ones to construct the noisy previous context for predicting the target sentence (replaced token denoising). In Figure 1,the generator predicts the masked words “guardian watered”, where the incorrect token “guardian” and correct token “watered” are both misclassified into REPLACED and ORIGINAL by the discriminator. Next, we resample a different token “watering” from the generated distribution. Consequently, the target tokens “gardener watered” are replaced with the sampled tokens “guardian watering” to construct the noisy sample. The generator predicts the next word conditioned on previous noisy tokens (replaced token denoising). Through combing two tasks, GANLM strengthen generation performance with the enhanced language understanding capability from the replaced token detection task.</p><p>​    本文提出一种预训练框架GANLM，用gan风格的学习来提高预训练语言模型的可迁移性，用于自然语言生成。具体来说，编码器读取被掩码的源语句，生成器获得目标分布。然后，鉴别器区分从目标分布中采样的每个token是否与目标黄金句子匹配(替换token检测)。判别器将误分类的词项视为硬词项，供生成器进行准确预测。我们将目标句子中的原始标记替换为错误分类的采样标记，以构建含噪的前一个上下文来预测目标句子(替换标记去噪)。在图1中，生成器预测了被屏蔽的单词” guardian “，其中不正确的标记” guardian “和正确的标记” “都被判别器错误地分类为替换标记和原始标记。接下来，我们从生成的分布中重新采样不同的token “浇水”。因此，将目标标记” gardener “替换为采样标记” guardian “来构建噪声样本。生成器根据之前的噪声标记(替换的标记去噪)预测下一个单词。通过将两个任务相结合，GANLM增强了生成性能，并增强了替换token检测任务的语言理解能力。</p><p>​    Our method is effective for text generation and can be extended to natural language understanding tasks. We pre-train GANLM model on large-scale monolingual corpora and evaluate the performance of our pre-trained English model GANLM and multilingual model GANLM-m on various downstream tasks, including text summarization, machine translation, and data-to-text generation. Experimental results demonstrate that our method substantially outperforms previous pre-trained encoder and sequence-to-sequence models on generation tasks. Our method is further tested on GLUE (Wang et al.,\2019) and XNLI (Conneau et al., 2018) to validate the transferability of our pre-trained model. Analytic experiments emphasize the importance of the discriminator in both the pre-training and finetuning stage, leading to better performance.</p><p>​    该方法对文本生成是有效的，可以扩展到自然语言理解任务。在大规模单语语料库上预训练了GANLM模型，并评估了预训练英语模型GANLM和多语言模型GANLM-m在各种下游任务上的性能，包括文本摘要、机器翻译和数据到文本生成。实验结果表明，该方法在生成任务上大大优于之前的预训练编码器和序列到序列模型。我们的方法在GLUE (Wang et al.，\2019)和XNLI (Conneau et al.， 2018)上进行了进一步测试，以验证预训练模型的可移植性。分析性实验强调了判别器在预训练和微调阶段的重要性，从而获得了更好的性能。</p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231127231837770.png" align="center"></p><p>Figure 1: A pre-training sample of our method, where replaced token detection (discriminator) and replaced token denoising (generator) are used for pre-training. The discriminator classifies each generated token into REPLACED or ORIGINAL, where REPLACED denotes the predicted token is different from the gold token. The red fonts denote incorrect predictions.</p><p>图1:我们方法的预训练样本，其中替换的token检测(鉴别器)和替换的token去噪(生成器)用于预训练。鉴别器将每个生成的token分类为替换的或原始的，替换表示预测的token不同于黄金token。红色字体表示错误的预测。</p><h2><span id="2-ganlm">2 GANLM</span></h2><h3><span id="21-model-overview">2.1 Model Overview</span></h3><p>Our GAN-style pre-trained model comprises a generator (<em>G</em>) and discriminator (<em>D</em>), which are both encoder-decoder frameworks and conditioned on the same encoder (Enc). In Figure 2, the encoder reads the masked sentence and the generator decoder obtains the target distribution. Then the discriminator decoder distinguishes whether each token in the sampled target sentence matches the gold reference. Tokens in the target gold sentence are randomly replaced with misclassified ones by the discriminator to construct the noisy sample, which is fed into the generator decoder to predict the target sentence (replaced token denoising).</p><p>我们的生成对抗网络（GAN）风格预训练模型包括一个生成器 (<em>G</em>) 和一个判别器 (<em>D</em>)，它们都是编码器-解码器框架，并且都在相同的编码器（Enc）的条件下。在图2中，编码器读取被掩码的句子，生成器解码器获取目标分布。然后，判别器解码器区分采样目标句子中的每个标记是否与黄金参考匹配。目标黄金句子中的标记会被判别器随机替换为被错误分类的标记，以构建含噪样本，然后将其输入生成器解码器以预测目标句子（替换标记去噪）。</p><h3><span id="22-masked-sequence-generator">2.2 Masked Sequence Generator</span></h3><p>Given a monolingual sentence $x = (x_1,\cdots,x_n)$ with $n$ words from the dataset $D_k$ of language $L_k \in L_{all}=\{L_1,\cdots,L_K\}(|L_{all}|=K)$, some random spans of contiguous tokens in $x$ corrupted as the source sentence, which is denoted as $x^{src}=(x_1,\cdots,x_{\backslash u:v},\cdots,x_n)$. $x_{\backslash u:v}$ is a masked span of $x_{u:v}$, where the fragment from position $u$ to $v$ is corrupted by [MASK]. Given $x^{src}$, the generator predicts the original identities of the masked tokens $x^{trg}=(x_{\backslash 1},\cdots,x_{\backslash u:v},\cdots,x_{\backslash n})$ autoregressively:</p><script type="math/tex; mode=display">x^{trg}_t=Enc-Dec(x^{src},x^{trg}_{1:t-1};\{\theta_{\epsilon},\theta_{\mathcal{G}}\})</script><p>where $\theta_{\epsilon}$  and $\theta_{\mathcal{G}}$ denote the encoder and decoder parameters of the generator. Enc-Dec denotes an encoder-decoder model. The generator predicts the next position $t$ token $x^{trg}_t$ based on previous tokens. </p><p>给定一个单语句 $x = (x_1,\cdots,x_n)$，其中包含来自语言 $L_k$ 的数据集 $D_k$ 中的 $n$ 个单词，其中 $L_k \in L_{all}=\{L_1,\cdots,L_K\}$（$|L_{all}|=K$），$x$ 的一些连续标记的随机跨度被损坏，作为源句子表示为 $x^{src}=(x_1,\cdots,x_{\backslash u:v},\cdots,x_n)$。$x_{\backslash u:v}$ 是 $x_{u:v}$ 的一个掩码跨度，其中从位置 $u$ 到 $v$ 的片段被 [MASK] 损坏。给定 $x^{src}$，生成器通过自回归方式预测被掩码的标记的原始标识 $x^{trg}=(x_{\backslash 1},\cdots,x_{\backslash u:v},\cdots,x_{\backslash n})$：</p><script type="math/tex; mode=display">x^{trg}_t=Enc-Dec(x^{src},x^{trg}_{1:t-1};\{\theta_{\epsilon},\theta_{\mathcal{G}}\}) \tag 1</script><p>其中 $\theta_{\epsilon}$ 和 $\theta_{\mathcal{G}}$ 表示生成器的编码器和解码器参数。Enc-Dec 表示一个编码器-解码器模型。生成器基于先前标记预测下一个位置 $t$ 的标记 $x^{trg}_t$。</p><p>​    The training objective of sequence-to-sequence masked language modeling (S2S-MLM) on the dataset $D_k$ of language $L_k$ is defined as:</p><script type="math/tex; mode=display">\mathcal{L_{\mathcal{G}}}=E_{x\sim{D_k}}[\log P_G(x^{trg}|x^{src};\{\theta_{\epsilon},\theta_{\mathcal{G}}\})]</script><p>where $x^{src}$ and $x^{trg}$ are derived from $x$.</p><p>​    语言 $L_k$ 的序列到序列掩码语言建模（S2S-MLM）的训练目标定义为：</p><script type="math/tex; mode=display">\mathcal{L_{\mathcal{G}}}=E_{x\sim{D_k}}[\log P_G(x^{trg}|x^{src};\{\theta_{\epsilon},\theta_{\mathcal{G}}\})] \tag 2</script><p>其中 $x^{src}$ 和 $x^{trg}$ 派生自 $x$。</p><h3><span id="23-replaced-token-detection">2.3 Replaced Token Detection</span></h3><p>The generator outputs the distribution of each target token and we create a sampled sentence $\hat{x}^{trg}$ by randomly sampling tokens from the distribution. The discriminator distinguishes whether each token in<br>$\hat{x}^{trg}$ is replaced compared to $x^{trg}$. Given the target distribution $P_G(x^{trg}_t|x^{src})(x^{trg}_t\in x^{src})$ from the generator, we construct $\hat{x}^{trg}$ for the discriminator:</p><script type="math/tex; mode=display">\hat{x}^{trg} = REPLACE(x^{trg};x'_t) \\w.r.t. x'_t \sim P_G(x^{trg}_t|x^{src})\and(x^{trg}_t\in x^{src}) \tag 3</script><blockquote><p>w.r.t. with respect to 关于，相对于</p></blockquote><p>生成器输出每个目标标记的分布，我们通过从分布中随机抽样标记创建一个样本句子 $\hat{x}^{trg}$。判别器区分在 $\hat{x}^{trg}$ 中的每个标记是否与 $x^{trg}$ 相比发生了替换。鉴于来自生成器的目标分布 $P_G(x^{trg}_t|x^{src})(x^{trg}_t\in x^{src})$，我们为判别器构建 $\hat{x}^{trg}$：</p><script type="math/tex; mode=display">\hat{x}^{trg} = REPLACE(x^{trg};x'_t) \\\text{w.r.t. } x'_t \sim P_G(x^{trg}_t|x^{src})\and(x^{trg}_t\in x^{src}) \tag{3}</script><p>where REPLACE($\cdot$) replaces target <em>t</em>-th position unmasked token in $x^{trg}$ with the sampled token $x’_t$<br>from the generated distribution $P_G(x^{trg}_t|x^{src})$. </p><p>其中，REPLACE($\cdot$) 替换 $x^{trg}$ 中目标第 <em>t</em> 位置的未掩码标记，用来自生成分布 $P_G(x^{trg}_t|x^{src})$ 的抽样标记 $x’_t$ 替换。</p><p>​    Given the source sentence $x^{src}$ and the encoder $\theta_{\epsilon}$ , the decoder of the discriminator $\theta_{\mathcal{D}}$ obtains a sequence of hidden representations $H_d=(h_1,\cdots,h_n)$ by feeding the sampled sentence $\hat{x}^{trg}$ to the discriminator decoder:</p><script type="math/tex; mode=display">H_d =Enc-Dec(x^{src},\hat{x}^{trg};\{\theta_{\epsilon},\theta_{\mathcal{D}}\}) \tag 4</script><p>​    鉴于源句子 $x^{src}$ 和判别器的编码器 $\theta_{\epsilon}$，判别器的解码器 $\theta_{\mathcal{D}}$ 通过将抽样句子 $\hat{x}^{trg}$ 提供给判别器解码器，获得一系列隐藏表示 $H_d=(h_1,\cdots,h_n)$：</p><script type="math/tex; mode=display">H_d = Enc-Dec(x^{src},\hat{x}^{trg};\{\theta_{\epsilon},\theta_{\mathcal{D}}\}) \tag{4}</script><p>where $\theta_{\epsilon}$ and $\theta_{\mathcal{D}}$ denote the encoder and decoder parameters of the discriminator. The decoder of the discriminator $\theta_{\mathcal{D}}$ adopts the bidirectional language model to classify each input token by extracting the past and future representations.</p><p>其中，$\theta_{\epsilon}$ 和 $\theta_{\mathcal{D}}$ 分别表示判别器的编码器和解码器参数。判别器的解码器 $\theta_{\mathcal{D}}$ 采用双向语言模型，通过提取过去和未来的表示来对每个输入标记进行分类。</p><p>​    Given the representations $H_d$, the discriminator classifies sampled tokens $\hat{x}^{trg}$ into the REPLACED or ORIGINAL label with a sigmoid function $\sigma$:</p><script type="math/tex; mode=display">V=\sigma(H_dW_d)\tag 5</script><p>where $W_d\in R^{d_\epsilon \times 2}$ is the matrix projects the token representations to two categories (REPLACED or ORIGINAL) and $d_\epsilon$ is the model hidden size.</p><p>​    鉴于表示 $H_d$，判别器使用 S 型函数 $\sigma$ 将抽样标记 $\hat{x}^{trg}$ 分类为 REPLACED 或 ORIGINAL 标签：</p><script type="math/tex; mode=display">V=\sigma(H_dW_d)\tag{5}</script><p>其中 $W_d\in R^{d_\epsilon \times 2}$ 是将标记表示投影到两个类别（REPLACED 或 ORIGINAL）的矩阵，$d_\epsilon$ 是模型的隐藏大小。</p><p>​    The training objective of the replaced token detection task for the discriminator is:</p><script type="math/tex; mode=display">\mathcal{L_{\mathcal{D}}}=E_{x\sim{D_k}}[\mathbb{I}(\hat{x}^{trg}=x^{trg})\log V+\mathbb{I}(\hat{x}^{trg}\neq x^{trg})\log(1-V)] \tag 6</script><p>where $\mathbb{I}(\cdot)$ is the indicator function.</p><p>​    判别器替换标记检测任务的训练目标为：</p><script type="math/tex; mode=display">\mathcal{L_{\mathcal{D}}}=E_{x\sim{D_k}}[\mathbb{I}(\hat{x}^{trg}=x^{trg})\log V+\mathbb{I}(\hat{x}^{trg}\neq x^{trg})\log(1-V)] \tag{6}</script><p>其中 $\mathbb{I}(\cdot)$ 是指示函数。</p><h3><span id="24-replaced-token-denoising">2.4 Replaced Token Denoising</span></h3><p>Although our model structure is similar to GAN,  the generator is trained with maximum likelihood rather than the standard GAN objective due to the difficulty of the GAN training in NLP. We replace tokens in $x^{trg}$ with misclassified tokens by discriminator to construct the noisy previous context $x^{trg}_f$. If the sampled token $\hat{x}^{trg}_t=x_t$  is labeled with ORIGINAL, we will resample the token $x’_t(x’_t\neq x_t)$ from target distribution as the misclassified token to modify $x_t$ in $x^{trg}$. When $\hat{x}^{trg}_t=x’_t(x’_t\neq x_t)$ is labeled with REPLACED, the miscalssified token $x’_t$ directly replaces $x_t$ in the target sentence. Given the target sentence $x^{trg}$ and generated probabilities $P_G$, we replace tokens in $x^{trg}$ with sampled tokens as the previous noisy context:</p><script type="math/tex; mode=display">x^{trg}_f = REPLACE(x^{trg};\hat{x}^{trg}_t) \\w.r.t. \ \hat{x}^{trg}_t \sim P_G(x^{trg}_t|x^{src})\and(t\in v) \tag{7}</script><p>where $v=\{v_1,\cdots,v_p\}(|v|=p)$ denotes the positions in $x^{trg}$ of the misclassified tokens. </p><p>尽管我们的模型结构类似于 GAN，由于在自然语言处理中 GAN 训练的困难，生成器是使用最大似然而不是标准 GAN 目标进行训练。我们通过判别器用被错误分类的标记替换 $x^{trg}$ 中的标记来构建带有噪声的先前上下文 $x^{trg}_f$。如果抽样的标记 $\hat{x}^{trg}_t=x_t$ 被标记为 ORIGINAL，我们将从目标分布中重新抽样标记 $x’_t(x’_t\neq x_t)$ 作为错误分类的标记，以修改 $x^{trg}$ 中的 $x_t$。当 $\hat{x}^{trg}_t=x’_t(x’_t\neq x_t)$ 被标记为 REPLACED 时，错误分类的标记 $x’_t$ 直接替换目标句子中的 $x_t$。给定目标句子 $x^{trg}$ 和生成的概率 $P_G$，我们用抽样的标记替换 $x^{trg}$ 中的标记，形成先前的带噪声上下文：</p><script type="math/tex; mode=display">x^{trg}_f = REPLACE(x^{trg};\hat{x}^{trg}_t) \\\text{w.r.t. } \hat{x}^{trg}_t \sim P_G(x^{trg}_t|x^{src})\text{ 且 }(t\in v) \tag{7}</script><p>其中 $v=\{v_1,\cdots,v_p\}(|v|=p)$ 表示 $x^{trg}$ 中被错误分类的标记的位置。</p><p>The training objective of the replaced token denoising ($\mathcal{DG}$) task based on the source sentence $x^{src}$ and target noisy context $x^{trg}_f$ is described as:</p><script type="math/tex; mode=display">\mathcal{L_{\mathcal{DG}}}=E_{x\sim{D_{L_k}}}[-\log P(x^{trg}|x^{src},x^{trg}_f;\{\theta_{\epsilon},\theta_{\mathcal{D}}\})]\tag{8}</script><p>where $x^{trg}$ is predicted by the previous noisy tokens $x^{trg}_f$ instead of previous gold context.</p><p>基于源句子 $x^{src}$ 和目标带噪声上下文 $x^{trg}_f$ 的替换标记去噪（$\mathcal{DG}$）任务的训练目标如下：</p><script type="math/tex; mode=display">\mathcal{L_{\mathcal{DG}}}=E_{x\sim{D_{L_k}}}[-\log P(x^{trg}|x^{src},x^{trg}_f;\{\theta_{\epsilon},\theta_{\mathcal{D}}\})] \tag{8}</script><p>其中 $x^{trg}$ 是由先前的带噪声标记 $x^{trg}_f$ 预测的，而不是先前的黄金上下文。</p><h3><span id="25-multi-task-learning">2.5 Multi-task Learning</span></h3><p>Given multilingual corpora $D_{all}=\{D_1,\cdots,D_K\}$ of $K$ languages, the pre-trained model with parameters $\{\theta_{\epsilon},\theta_{\mathcal{G}}, \theta_{\mathcal{D}}\}$ is jointly trained over <em>K</em> languages to optimize the combined self-supervised objective as below:</p><script type="math/tex; mode=display">\mathcal{L_P}=\mathbb{E}_{L_k\in L_{all}}[\mathcal{L_G}+\lambda\mathcal{L_D}+\mathcal{L_DG}] \tag{9}</script><p>where $\lambda=10.0$ is the discriminator weight and $L_{all}=\{L_1,\cdots,L_K\}$. To improve model efficiency, a tiny discriminator decoder (4 layers) is adopted to help the generator decoder (12 layers).</p><p>给定 $K$ 种语言的多语料库 $D_{all}=\{D_1,\cdots,D_K\}$，使用参数 $\{\theta_{\epsilon},\theta_{\mathcal{G}}, \theta_{\mathcal{D}}\}$ 的预训练模型通过在 <em>K</em> 种语言上联合训练以优化下面的综合自监督目标：</p><script type="math/tex; mode=display">\mathcal{L_P}=\mathbb{E}_{L_k\in L_{all}}[\mathcal{L_G}+\lambda\mathcal{L_D}+\mathcal{L_DG}] \tag{9}</script><p>其中 $\lambda=10.0$ 是判别器权重，$L_{all}=\{L_1,\cdots,L_K\}$。为提高模型效率，使用一个较小的判别器解码器（4 层）来辅助生成器解码器（12 层）。</p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231128100203852.png" alt="image-20231128100203852"></p><p>Figure 2: Overview of GANLM, including (a) replaced token detection and (b) replaced token denoising. The encoder reads the source sentence and the generator obtains target distribution, where the generator and discriminator are supervised by the gold labels in (a). The discriminator distinguishes whether the sampled tokens “guardian watered” are replaced (both tokens are misclassified in this example). For the correct predicted token “watered”, we obtain a different token “watering” by resampling. The target tokens are replaced with the misclassified tokens to construct the noisy input, which are used to predict the gold sentence “gardener watered [EOS]” in (b).</p><p><strong>图 2：GANLM概述</strong></p><p><strong>(a) 替换标记检测</strong> 和 <strong>(b) 替换标记去噪</strong></p><p>在 (a) 中，编码器读取源句子，生成器获取目标分布，生成器和判别器在此过程中由金标签进行监督。判别器区分采样标记“guardian watered”是否被替换（在此示例中，两个标记都被错误分类）。对于正确预测的标记“watered”，我们通过重新抽样获得一个不同的标记“watering”。目标标记被替换为错误分类的标记，构建带有噪声的输入，用于预测 (b) 中的金句子“gardener watered [EOS]”。</p><h2><span id="3-discriminator-enhanced-fine-tuning">3 Discriminator-enhanced Fine-tuning</span></h2><p>To fully utilize the pre-trained parameters, we keep the auxiliary discriminator in downstream generation tasks (discriminator-enhanced fine-tuning) to enhance the generator, where both the pre-trained generator and discriminator are recycled. Given the annotated corpus $D_s$ of $K$ languages, the pretrained model $\{\theta_{\epsilon},\theta_{\mathcal{G}}, \theta_{\mathcal{D}}\}$ is optimized by:</p><script type="math/tex; mode=display">\mathcal{L_F}=\mathbb{E}_{x,y\sim D_s}[\mathcal{L_G}+\lambda\mathcal{L_D}+\mathcal{L_DG}] \tag{10}</script><p>where $x$ and $y$ are the parallel pair from $D_s$. The objective in the fine-tuning stage use the original pair  $x$ and $y$ without S2S-MLM. The generator $\{\theta_{\epsilon},\theta_{\mathcal{G}}\}$ are kept for inference by throwing out the discriminator decoder $ \theta_{\mathcal{D}}$. Alternatively, the discriminator $(\mathcal{D}:\{\theta_{\epsilon},\theta_{\mathcal{D}}\})$ or generator $(\mathcal{G}:\{\theta_{\epsilon},\theta_{\mathcal{G}}\})$ can also be separately fine-tuned on the downstream task.</p><p>为了充分利用预训练参数，我们在下游生成任务中保留辅助判别器（判别器增强微调）以增强生成器，其中预训练的生成器和判别器都得到了重复利用。给定 $K$ 种语言的带标注语料库 $D_s$，预训练模型 $\{\theta_{\epsilon},\theta_{\mathcal{G}}, \theta_{\mathcal{D}}\}$ 通过以下方式进行优化：</p><script type="math/tex; mode=display">\mathcal{L_F}=\mathbb{E}_{x,y\sim D_s}[\mathcal{L_G}+\lambda\mathcal{L_D}+\mathcal{L_DG}] \tag{10}</script><p>其中 $x$ 和 $y$ 是来自 $D_s$ 的平行语料对。微调阶段的目标使用原始对 $x$ 和 $y$，而不使用 S2S-MLM。生成器 $\{\theta_{\epsilon},\theta_{\mathcal{G}}\}$ 被保留用于推断，而判别器解码器 $ \theta_{\mathcal{D}}$ 被丢弃。或者，判别器 $(\mathcal{D}:\{\theta_{\epsilon},\theta_{\mathcal{D}}\})$ 或生成器 $(\mathcal{G}:\{\theta_{\epsilon},\theta_{\mathcal{G}}\})$ 也可以在下游任务上分别进行微调。</p><h2><span id="4-experiment-setting">4 Experiment Setting</span></h2><h3><span id="41-pre-training-details">4.1 Pre-training Details</span></h3><h4><span id="model-configuration">Model Configuration</span></h4><p>In the experiments, we adopt a sequence-to-sequence base-setting Transformer architecture with 768 hidden size, 3072 FFN (feed-forward network) dimension, 12 attention heads, and 12 encoder/decoder layers. The maximum sequence length of learned positions embeddings in the encoder/decoder is set as 1024. All token embedding matrices and output projection matrix parameters are shared for model efficiency. </p><h4><span id="模型配置">模型配置</span></h4><p>在实验中，我们采用了一个基于序列到序列的 Transformer 架构，具有 768 的隐藏大小，3072 的前馈网络（FFN）维度，12 个注意力头，以及 12 个编码器/解码器层。在编码器/解码器中学到的位置嵌入的最大序列长度被设置为 1024。为了提高模型效率，所有标记嵌入矩阵和输出投影矩阵参数都是共享的。</p><h4><span id="dataset">Dataset</span></h4><p>Following the previous work (Liu et al., 2019), our English pre-trained model GANLM is</p><p>trained on 160GB English monolingual data from BookCorpus, CC-News, OpenWebText, and CC Stories. In addition, we pre-train GANLM-m with 6TB multilingual data as the pioneering work (Maet al., 2021), which is a combination of CC100, CCNet, and Wikipedia, covering 100 languages. All texts are tokenized by SentencePiece (Kudo and Richardson, 2018) and encoded by the dictionary from XLM-R (Conneau et al., 2020).</p><h4><span id="数据集">数据集</span></h4><p>按照先前的工作（Liu et al., 2019），我们的英语预训练模型 GANLM 是在来自 BookCorpus、CC-News、OpenWebText 和 CC Stories 的 160GB 英语单语数据上训练的。此外，我们还预训了 GANLM-m，使用了 6TB 多语言数据，这是作为开创性工作（Ma et al., 2021）的一部分，涵盖了CC100、CCNet 和 Wikipedia，涵盖了 100 种语言。所有文本都由 SentencePiece（Kudo 和 Richardson, 2018）进行分词，并由 XLM-R（Conneau et al., 2020）的字典进行编码。</p><h4><span id="optimization">Optimization</span></h4><p>For S2S-MLM, we randomly mask 15% of the words in each instance with an average span length of 3 (Raffel et al., 2020). For the replaced token detection, we set the discriminator weight $\lambda=10.0$. We adopt Adam (Kingma and Ba, 2015) with a learning rate of 3e-4 and 10K warm-up steps for pre-training. The model is trained on 128 NVIDIA A100 GPUs (40GB) from scratch and each batch contains 8K samples. The English pre-trained model GANLM and multilingual model GANLM-m are trained for 500K steps. Specifically, all methods in Table 1 are pre-trained with 500K steps for a fair comparison.</p><h4><span id="优化">优化</span></h4><p>对于 S2S-MLM，我们随机地对每个实例中的 15% 单词进行屏蔽，平均跨度长度为 3（Raffel et al., 2020）。对于替换标记检测，我们设置判别器权重 $\lambda=10.0$。我们采用 Adam（Kingma 和 Ba, 2015）作为优化器，学习率为 3e-4，并进行了 10K 次热身步骤进行预训练。该模型是在 128 个 NVIDIA A100 GPU（40GB）上从头开始训练的，每个批次包含 8K 个样本。英语预训练模型 GANLM 和多语言模型 GANLM-m 训练了 500K 步。具体而言，表格1中的所有方法都进行了 500K 步的预训练，以进行公平比较。</p><h3><span id="42-downstream-tasks">4.2 Downstream Tasks</span></h3><h4><span id="monolingual-summarization">Monolingual Summarization</span></h4><p><strong>CNN / DailyMail</strong>(See et al., 2017) is an abstractive summarization dataset aiming at generating a concise summary from an English news article in CNN and DailyMail. As a popular abstractive summarization dataset, <strong>XSum</strong> (Narayan et al., 2018) compresses a BBC news article to a short one-sentence summary.</p><h4><span id="单语言摘要">单语言摘要</span></h4><p><strong>CNN/DailyMail</strong>（See et al., 2017）是一个目标生成摘要的抽象摘要数据集，旨在从 CNN 和 DailyMail 的英语新闻文章中生成简洁的摘要。作为一个受欢迎的抽象摘要数据集，<strong>XSum</strong>（Narayan et al., 2018）将 BBC 新闻文章压缩成一个简短的一句摘要。</p><h4><span id="multilingual-summarization">Multilingual Summarization</span></h4><p>To test the capability of our multilingual pre-trained model, a large-scale multilingual dataset named  <strong>WikiLingua</strong> (Ladhak et al., 2020) of 18 languages from WikiHow is used to evaluate multilingual abstractive summarization systems.</p><h4><span id="多语言摘要">多语言摘要</span></h4><p>为了测试我们多语言预训练模型的能力，我们使用了一个名为<strong>WikiLingua</strong>（Ladhak et al., 2020）的大规模多语言数据集，其中包含来自 WikiHow 的 18 种语言，用于评估多语言抽象摘要系统。</p><h4><span id="bilingual-translation">Bilingual Translation</span></h4><p>For the bilingual task, we use the <strong>WMT-14 English-German</strong>, <strong>WMT-14 English-French</strong>, and <strong>WMT-16 EnglishRomanian</strong> dataset for evaluation. WMT-14 En-De from WMT consists of 4.5M sentence pairs and the newstest2014 is used as the test set. WMT-14 EnFr is a large-scale dataset containing nearly 41M sentence pairs and newstest2014 is adopted for evaluation. WMT-16 En-Ro is comprised of original parallel sentences and back-translation data.</p><h4><span id="双语翻译">双语翻译</span></h4><p>对于双语任务，我们使用<strong>WMT-14英德</strong>、<strong>WMT-14英法</strong>和<strong>WMT-16英罗</strong>数据集进行评估。WMT-14 En-De 来自 WMT，包含 450 万个句对，newstest2014 用作测试集。WMT-14 En-Fr 是一个大规模数据集，包含近 4100 万个句对，采用 newstest2014 进行评估。WMT-16 En-Ro 由原始平行句子和反向翻译数据组成。</p><h4><span id="multilingual-translation">Multilingual Translation</span></h4><p><strong>IWSLT-17</strong> of 5 languages and <strong>WMT-10</strong> of 11 languages are utilized for multilingual translation. For IWSLT-17, English (En), German (De), Italian (It), Dutch (Nl), and Romanian (Ro) corpora are downloaded from the IWSLT-2017 benchmark. We use dev2010 for validation and tst2017 for test. For WMT-10, we use the parallel data of 11 languages from the WMT benchmark for evaluation (Wang et al., 2020).</p><h4><span id="多语言翻译">多语言翻译</span></h4><p>我们利用包含 5 种语言的<strong>IWSLT-17</strong>和包含 11 种语言的<strong>WMT-10</strong>进行多语言翻译。对于 IWSLT-17，我们从 IWSLT-2017 基准下载了英语（En）、德语（De）、意大利语（It）、荷兰语（Nl）和罗马尼亚语（Ro）语料库。我们使用 dev2010 进行验证，tst2017 进行测试。对于 WMT-10，我们使用 WMT 基准的 11 种语言的平行数据进行评估（Wang et al., 2020）。</p><h4><span id="data-to-text-generation">Data-to-Text Generation</span></h4><p>Data-to-text generation accepts multiple triplets and produces a description. WebNLG (Gardent et al., 2017) contains parallel DBpedia triple sets and short texts. The EnEn direction contains 17K triple sets and 45K short texts and the En-Ru direction contains 7K triple sets and 19K texts in Russian. The ROUGE scores on the valid set are reported for a fair comparison with the previous work (Gehrmann et al., 2021).</p><h4><span id="数据生成文本">数据生成文本</span></h4><p>数据生成文本接受多个三元组并生成描述。<strong>WebNLG</strong>（Gardent et al., 2017）包含平行的 DBpedia 三元组集和短文本。英英方向包含 17K 个三元组集和 45K 个短文本，而英俄方向包含 7K 个三元组集和 19K 个俄文文本。为了与先前的工作（Gehrmann et al., 2021）进行公平比较，报告了验证集上的 ROUGE 分数。</p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231128104227297.png" alt="image-20231128104227297"></p><p>Table 1: Comparison of different pre-training objectives. Particularly, all methods in this table use the base-setting model and are pre-trained with 500K steps on the same corpora for a fair comparison. We report ROUGE scores for abstractive text summarization (XSum) and BLEU scores for multilingual machine translation (IWSLT-17).</p><p>表1：不同预训练目标的比较。特别地，表中所有方法使用基础设置模型，并在相同的语料库上进行了50万步的预训练，以进行公平比较。我们报告了对抽象文本摘要（XSum）的ROUGE分数以及对多语言机器翻译（IWSLT-17）的BLEU分数。</p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231128104355560.png" alt="image-20231128104355560"></p><p>Table 2: Abstractive summarization results on the test set of CNN / DailyMail, and XSum. The evaluation metric is the F1 score of ROUGE (RG) scores.</p><p>表2：在CNN / DailyMail和XSum的测试集上的抽象摘要结果。评估指标是ROUGE（RG）分数的F1分数。</p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231128104426541.png" alt="image-20231128104426541"></p><p>Table 3: Results of our method and other baselines on multilingual abstractive summarization. We report the RG-1/RG-2/RG-L (ROUGE) F1 scores of the 18 WikiLingua languages and the average scores.</p><p>表3：我们的方法和其他基线在多语言抽象摘要上的结果。我们报告了对18种WikiLingua语言的ROUGE-1/ROUGE-2/ROUGE-L（ROUGE）F1分数以及平均分数。</p><h3><span id="43-fine-tuning-details">4.3 Fine-tuning Details</span></h3><h4><span id="abstractive-summarization">Abstractive Summarization</span></h4><p>During fine-tuning, we use the Adam (Kingma and Ba, 2015) optimizer with an initial learning rate of 1e-4 and the batch size is set as 2048 tokens on 8 V100 GPUs. The models are trained with the label smoothing cross entropy with a smoothing ratio of 0.1.</p><h4><span id="抽象摘要">抽象摘要</span></h4><p>在微调过程中，我们使用Adam优化器（Kingma和Ba，2015），初始学习率为1e-4，批大小设置为在8个V100 GPU上为2048个标记。模型使用标签平滑的交叉熵进行训练，平滑比例为0.1。</p><h4><span id="neural-machine-translation">Neural Machine Translation</span></h4><p>For the large-scale multilingual dataset WMT-10, our pre-trained model is fine-tuned on 32 V100 GPUs with a learning rate of 3e-4. For all bilingual translation tasks and the IWSLT-2017 benchmark, we adopt Adam with a learning rate of 1e-4 and set the batch size as 2048 tokens on 8 V100 GPUs.</p><h4><span id="神经机器翻译">神经机器翻译</span></h4><p>对于大规模多语言数据集WMT-10，我们的预训练模型在32个V100 GPU上进行微调，学习率为3e-4。对于所有的双语翻译任务和IWSLT-2017基准测试，我们采用Adam优化器，学习率为1e-4，并在8个V100 GPU上将批大小设置为2048个标记。</p><h4><span id="data-to-text-generation">Data-to-text Generation</span></h4><p>We use Adam with a learning rate of {8e-5,1e-4} and set the batch size as 16 sentences on the WebNLG dataset.</p><h4><span id="数据到文本生成">数据到文本生成</span></h4><p>在WebNLG数据集上，我们使用Adam优化器，学习率为{8e-5, 1e-4}，批大小设置为16个句子。</p><h2><span id="5-comparing-pre-training-objectives">5 Comparing Pre-training Objectives</span></h2><p>To verify the potential of our pre-training task under a fair comparison, we re-implement previous pre-training tasks and pre-trains baselines on the same corpora with 500K steps, including BERT/mBERT (Devlin et al., 2019), ELECTRA (Clark et al., 2020), BART (Lewis et al., 2020)/mBART (Liu et al., 2020), and T5 (Raffel et al.,2020)/mT5 (Xue et al., 2021). Table 1 reports the ROUGE and BLEU points on the summarization dataset XSum and multilingual translation dataset IWSLT-17. All models have 12 encoder and 12 decoder layers with a hidden size of 768. We observe that the encoder-decoder pre-trained model (T5/mT5) outperforms the pre-trained encoder (ELECTRA, BERT/mBERT), which corroborates the encoder-decoder pre-training is morebeneficial to the downstream generation task. Experiments ⑥<em>∼</em>⑧ show the importance of the discriminator and replaced token denoising. Experiment ⑧ demonstrates that only the replaced token detection task can still bring improvement through strengthening the encoder shared by both generator and discriminator. Besides, the replaced token detection task is also helpful to downstream language understanding tasks with a powerful encoder. Lastly, the results verify that fine-tuning with the help of the pre-trained auxiliary discriminator further improves performance.</p><p>为了在公平比较下验证我们预训练任务的潜力，我们重新实现了以前的预训练任务，并在相同的语料库上进行了500K步的预训练，包括BERT/mBERT（Devlin等人，2019）、ELECTRA（Clark等人，2020）、BART（Lewis等人，2020）/mBART（Liu等人，2020）和T5（Raffel等人，2020）/mT5（Xue等人，2021）。表1报告了在摘要数据集XSum和多语言翻译数据集IWSLT-17上的ROUGE和BLEU分数。所有模型都有12个编码器层和12个解码器层，隐藏大小为768。我们观察到编码器-解码器预训练模型（T5/mT5）优于仅进行编码器预训练的模型（ELECTRA、BERT/mBERT），这证实了编码器-解码器预训练对下游生成任务更有益。实验⑥<em>∼</em>⑧展示了鉴别器和替换标记去噪的重要性。实验⑧表明，仅替换标记检测任务仍然可以通过强化生成器和鉴别器共享的编码器来带来改善。此外，替换标记检测任务对具有强大编码器的下游语言理解任务也有帮助。最后，结果验证了在辅助预训练鉴别器的帮助下进行微调能够进一步提高性能。</p><h2><span id="6-results-of-ganlm">6 Results of GANLM</span></h2><p>The English pre-trained model GANLM is evaluated on the abstractive text summarization task with the ROUGE (Lin, 2004) scores.</p><p>英文预训练模型 GANLM 在抽象文本摘要任务上通过 ROUGE（Lin, 2004）分数进行评估。</p><h3><span id="xsum">XSum</span></h3><p>As shown in Table 2, the pre-training methods achieve significant improvements over the strong baseline PTRNET without pre-training. The sequence-to-sequence pre-trained model such as UniLMv2 + <em>s2s-ft</em> outperforms other pre-training baselines, where the pseudo-masked technique is applied to the fine-tuning stage. Our method beats all pre-training baselines by a large margin with the discriminator-enhanced fine-tuning strategy. It emphasizes the importance of the fine-tuning strategy for the performance of downstream tasks.</p><p>如表2所示，预训练方法在没有预训练的强基准模型 PTRNET 上取得了显著的改进。诸如 UniLMv2 + <em>s2s-ft</em> 这样的序列到序列预训练模型在精调阶段应用了伪掩码技术，优于其他预训练基准。我们的方法通过鉴别器增强的微调策略在所有预训练基准上大幅领先。这强调了微调策略对下游任务性能的重要性。</p><h3><span id="cnn-dailymail">CNN / DailyMail</span></h3><p>Our method is also evaluated on the CNN / DailyMail dataset in Table 2. The comparisons further indicate that our method obtains strong performance on generation by leveraging the discriminator.</p><p>我们的方法还在CNN / DailyMail数据集上进行了评估，如表2所示。进一步的比较表明，我们的方法通过利用鉴别器在生成任务上取得了强大的性能。</p><h2><span id="7-results-of-ganlm-m">7 Results of GANLM-m</span></h2><p>To evaluate the multilingual pre-trained model GANLM-m, we report the BLEU (Papineni et al., \2002) scores for machine translation and ROUGE (Lin, 2004) scores for text summarization and data to-text generation.</p><p>为了评估多语言预训练模型 GANLM-m，我们报告了机器翻译的 BLEU（Papineni等人，2002）分数，以及文本摘要和数据到文本生成的 ROUGE（Lin, 2004）分数。</p><h3><span id="wikilingua">WikiLingua</span></h3><p>Table 3 reports the average ROUGE scores of 18 WikiLingua languages. The large improvement over other pre-training method demon strate the summarization ability of our GANLM-m.</p><p>表3报告了18种 WikiLingua 语言的平均 ROUGE 分数。与其他预训练方法相比的巨大改进表明了我们的 GANLM-m 的摘要能力。</p><h3><span id="wmt14-en-de">WMT14 En-De</span></h3><p>The results on the bilingual translation are presented at Table 4. We observe that the proposed GANLM outperforms all previous works in the high-resource machine translation scenario (<em>&gt;</em> 4M sentence pairs).</p><p>双语翻译的结果显示在表4中。我们观察到，在高资源机器翻译场景（*&gt; 4M 句对）中，我们提出的 GANLM 胜过了所有先前的工作。</p><h3><span id="wmt14-en-fr">WMT14 En-Fr</span></h3><p>We further conduct experiments on the WMT14 En-Fr bilingual translation task. Table 4 GANLM-m shows that GANLM-m still brings significant improvement to the downstream task with large-scale machine translation fine tuning data (<em>&gt;</em> 40M sentence pairs).</p><p>我们在WMT14 En-Fr双语翻译任务上进一步进行实验。表4中的 GANLM-m 显示，GANLM-m 仍然在大规模机器翻译微调数据（*&gt; 40M 句对）上为下游任务带来显著的改进。</p><h3><span id="wmt16-en-ro">WMT16 En-Ro</span></h3><p>For the low-resource setting (&lt;1M sentence pairs), there is an average gain of +4 BLEU points compared to the Transformer baseline in Table 5. With the same back-translation data, GANLM-m further improves the model performance and still beats other baselines.</p><p>在低资源设置（&lt;1M 句对）中，与表5中的 Transformer 基线相比，平均 BLEU 分数提高了 +4 分。在相同的回译数据情况下，GANLM-m 进一步提高了模型性能，并仍然击败了其他基线。</p><h3><span id="wmt-10">WMT-10</span></h3><p>For the multilingual translation, we compare GANLM-m with the strong multilingual pre-trained models in Table 7 and Table 6, such as mBART (Liu et al., 2020). It is notable our method outperforms large pre-trained model mBART with 1024 hidden size by a large margin (+1<em>∼</em>2 BLEU points). Plus, there is a +1.5 BLEU gain over XLMR, whose encoder and decoder are initialized by the cross-lingual pre-trained encoder (Ma et al., 2020).</p><p>对于多语言翻译，我们在表7和表6中与强大的多语言预训练模型进行比较，如 mBART（Liu等人，2020）。值得注意的是，我们的方法在 BLEU 分数上大幅胜过具有 1024 隐藏大小的大型预训练模型 mBART（+1<em>∼</em>2 BLEU 分数）。此外，与由跨语言预训练编码器初始化的 XLM-R 相比，还有 +1.5 BLEU 的提升。</p><h3><span id="webnlg">WebNLG</span></h3><p>Table 8 presents the performance on the data-to-text generation task, showing that GANLM outperforms multilingual sequence-to sequence pre-training baselines mBART and mT5 by +2 ROUGE-L points on both languages.</p><p>表8展示了在数据到文本生成任务上的性能，表明 GANLM 在两种语言上都在 ROUGE-L 分数上优于多语言序列到序列预训练基线 mBART 和 mT5（+2 ROUGE-L 分数）。</p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231128105705309.png" alt="image-20231128105705309"></p><p>Table 4: Comparison with other pre-training approaches on the WMT14 En-De and WMT14 En-Fr benchmark.</p><p>表4：在WMT14 En-De和WMT14 En-Fr基准上与其他预训练方法的比较。</p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231128105718279.png" alt="image-20231128105718279"></p><p>Table 5: Comparison with other pre-training methods on the WMT16 En-Ro benchmark.<br>表5：在WMT16 En-Ro基准上与其他预训练方法的比较。</p><h2><span id="8-analysis">8 Analysis</span></h2><p><hr><br>版权信息</p>]]></content:encoded>
      
      
      
      
      <comments>http://www.warmfire.com/2023/11/27/GANLM-Encoder-Decoder-Pre-training-with-an-Auxiliary-Discriminator/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>机器学习学习记录</title>
      <link>http://www.warmfire.com/2023/11/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</link>
      <guid>http://www.warmfire.com/2023/11/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</guid>
      <pubDate>Mon, 27 Nov 2023 00:40:24 GMT</pubDate>
      
      <description>&lt;p&gt;0x00 前言&lt;/p&gt;
&lt;p&gt;机器学习与深度学习&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformer模型&lt;/li&gt;
&lt;li&gt;预处理概念&lt;/li&gt;
&lt;li&gt;T5模型&lt;/li&gt;
&lt;/ul&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>0x00 前言</p><p>机器学习与深度学习</p><ul><li>Transformer模型</li><li>预处理概念</li><li>T5模型</li></ul><span id="more"></span><h2><span id="0x01-transformer">0x01 Transformer</span></h2><h3><span id="overview">Overview</span></h3><ul><li><code>Transformer</code> is a model proposed by Google in 2017 for machine translation.</li><li>The inside of a Transformer is essentially an <code>Encoder-Decoder</code> structure.</li><li>The whole network structure is entirely composed of <code>Attention</code> mechanism.</li><li>Using a 6-layer <code>Encoder-Decoder</code> structure.</li><li><img src="https://pic2.zhimg.com/80/v2-d04206d1a624dcaec8d56c79bea053b1_720w.webp" alt="img"></li></ul><h3><span id="example">Example</span></h3><ul><li>For ease of understanding, we only look at one of the Encoder-Decoder structures.</li><li><img src="https://pic3.zhimg.com/80/v2-6a52fadb8ba310a12076aac79baa2be2_720w.webp" alt="img"></li><li>The encoder is responsible for mapping the language sequence into a hidden layer (step 2 above), which is a mathematical representation of the natural language sequence.</li><li>The decoder remaps the hidden layers into natural language sequences, which allows us to solve various problems such as sentiment analysis, machine translation, summary generation, semantic relation extraction, etc.</li><li>Briefly, what does each step of the diagram do:<ol><li>Input a natural language sequence to the encoder: <code>Why do we work</code>(<code>为什么要工作</code>);</li><li>The hidden layer output from the encoder is fed to the decoder;</li><li>Input the <code>&lt; &gt;</code> as the start symbol to decoder;</li><li>The decoder gets the first word “为”;</li><li>Input the “为” to decoder;</li><li>The decoder gets the second word “什”;</li><li>The process is repeated until the end symbol is printed.</li></ol></li></ul><p>The content about encoder is divided into 4 parts, which are explained in turn.</p><ul><li><img src="https://pic4.zhimg.com/80/v2-af307443f765f2ecf440534d76f7ba2b_720w.webp" alt="img"></li></ul><h3><span id="positional-encoding"><strong>Positional Encoding</strong></span></h3><ul><li><p>Input data X with dimensions <code>[batch size, sequence length]</code>, like <code>我们为什么工作</code></p><ul><li><p><code>batch size</code> is the size of batch, <code>sequence length</code> is the length of sentence. </p></li><li><p>i.e. if X=[“我们为什么工作”], batch size equals 1 and sequence length equals 7.</p></li><li>Briefly, it is the conversion of text -&gt; word vectors.</li></ul></li><li><p>Output $X_{embedding}$ with dimensions <code>[batch size, sequence length, embedding dimension]</code>.</p><ul><li>The size of embedding dimension is determined by the <code>Word2Vec</code> algorithm, <code>Tramsformer</code> uses a word vector of <code>512</code> length.</li><li>if X=[“我们为什么工作”], batch size equals 1, sequence length equals 7 and embedding dimension equals 512.</li></ul></li><li><p><img src="https://pic4.zhimg.com/80/v2-5deb991462e2794a79772bbf5cd714d7_720w.webp" alt="img"></p></li><li><p>The position of the text is important. In order to retain this location information for the Transformer to learn, we need to use <code>location embedding</code>.</p></li><li><p><code>Transformer</code> uses the sin-cos rule.</p><ul><li>$PE_{(pos,2i)}=\sin(pos/10000^{2i/d_{model}})$</li><li>$PE_{(pos,2i+1)}=\cos(pos/10000^{2i/d_{model}})$</li><li><code>pos</code> refers to the position of the word in the sentence, ranging from [0,h), i refers to the dimension of the word embedding, and the range is $[0,d_{model})$.</li></ul></li><li><p>Then unique texture location information is generated, and the model learns the dependencies between locations and the temporal characteristic of natural language.</p></li><li><p>Finally,  $X_{embedding}$ and position embeddings are added and sent to the next layer.</p></li><li><p>```python</p><h1><span id="导入依赖库">导入依赖库</span></h1><p>import numpy as np<br>import matplotlib.pyplot as plt<br>import seaborn as sns<br>import math</p><p>def get_positional_encoding(max_seq_len, embed_dim):</p><pre><code># 初始化一个positional encoding# embed_dim: 字嵌入的维度# max_seq_len: 最大的序列长度positional_encoding = np.array([    [pos / np.power(10000, 2 * i / embed_dim) for i in range(embed_dim)]    if pos != 0 else np.zeros(embed_dim) for pos in range(max_seq_len)])positional_encoding[1:, 0::2] = np.sin(positional_encoding[1:, 0::2])  # dim 2i 偶数positional_encoding[1:, 1::2] = np.cos(positional_encoding[1:, 1::2])  # dim 2i+1 奇数# 归一化, 用位置嵌入的每一行除以它的模长# denominator = np.sqrt(np.sum(position_enc**2, axis=1, keepdims=True))# position_enc = position_enc / (denominator + 1e-8)return positional_encoding</code></pre><p>positional_encoding = get_positional_encoding(max_seq_len=100, embed_dim=16)<br>plt.figure(figsize=(10,10))<br>sns.heatmap(positional_encoding)<br>plt.title(“Sinusoidal Function”)<br>plt.xlabel(“hidden dimension”)<br>plt.ylabel(“sequence length”)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">### **Self-attention layer**</span><br><span class="line"></span><br><span class="line">1. ![img](https://pic3.zhimg.com/80/v2-fa1f2a80f5c311fc2a51e8c1d37655b2_720w.webp)</span><br><span class="line"></span><br><span class="line">- The meaning of multi head is that the resulting matrix of $QK^T$ is called the attention matrix, and it shows how similar each word is to other words.</span><br><span class="line">- The larger the dot product, the closer the two vectors are.</span><br><span class="line">- ![img](https://pic4.zhimg.com/80/v2-3d94e8346052a261eed3356c46f05fa7_720w.webp)</span><br><span class="line">- Our goal is to have each word contain information about all the words in the current sentence, and with the attention layer, we do this. </span><br><span class="line">- It is important to note that the above `self attention` calculation in the process, we usually use `mini batch`, it is also more than a calculation of words, the above example in a sentence.</span><br><span class="line">- The length of each sentence is not the same and needs to be treated uniformly according to the length of the longest sentence. For short sentences, do the `Padding` operation, generally we use `0` to fill.</span><br><span class="line">- ![图片](https://mmbiz.qpic.cn/mmbiz_png/v1JN0W4OpXjr1iaicSWjfKiasqX6Af1z4ibPGzLrU09tucAobKyD7hDibroHYeAuLkPgpTUHUh17o8mw5aHMKwTvKow/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1)</span><br><span class="line"></span><br><span class="line">### **Residual linkage and layer normalization**</span><br><span class="line"></span><br><span class="line">- The residual design and layer normalization operations are added to prevent the gradient from disappearing and speed up the convergence.</span><br><span class="line"></span><br><span class="line">1. **residual design**</span><br><span class="line"></span><br><span class="line">   - We got in the previous step after attention matrix weighted `V`, namely `Attention(Q, K, V)`, we transpose it, making its and $X_&#123;embedding&#125;$ dimension is consistent, i.e. `[batch size, sequence length, embedding dimension]`, then we add them together to do the residual join, which adds the elements directly because they have the same dimensions.</span><br><span class="line">   - $X_&#123;embedding&#125;+Attention(Q, K,V)$</span><br><span class="line">   - In the following operation, after each module operation, the value before the operation and the value after the operation should be added to obtain the residual connection, so that the gradient can directly take a shortcut to the initial layer during training.</span><br><span class="line">   - $X+SubLayer(X)$</span><br><span class="line"></span><br><span class="line">2. **layer normalization**</span><br><span class="line"></span><br><span class="line">   - It normalizes the hidden layers of the neural network to standard normal distribution, namely `i.i.d` independent identically distributed, to speed up the training, the role of the accelerating convergence.</span><br><span class="line"></span><br><span class="line">   - $$</span><br><span class="line">     \mu_i=\frac&#123;1&#125;&#123;m&#125;\sum^m_&#123;i=1&#125;x_&#123;ij&#125;</span><br><span class="line">     $$</span><br><span class="line"></span><br><span class="line">     on the type of line in a matrix(row) average for the unit.</span><br><span class="line"></span><br><span class="line">   - $$</span><br><span class="line">     \sigma^2_j=\frac&#123;1&#125;&#123;n&#125;\sum^m_&#123;i=1&#125;(x_&#123;ij&#125;-\mu_j)^2</span><br><span class="line">     $$</span><br><span class="line"></span><br><span class="line">     on the type of line in a matrix(row) as the unit of variance.</span><br><span class="line"></span><br><span class="line">   - $$</span><br><span class="line">     LayerNorm(x)=\alpha\odot\frac&#123;x_&#123;ij&#125;-\mu_i&#125;&#123;\sqrt&#123;\sigma^2_i+\epsilon&#125;&#125;+\beta</span><br><span class="line">     $$</span><br><span class="line"></span><br><span class="line">     We introduce two trainable parameters to compensate for the information lost in the normalization process. Note that $\odot$ represents element-wise multiplication rather than dot  product, and we typically initialize $\alpha$ with all ones, and $\beta$ with all zeros.</span><br><span class="line"></span><br><span class="line">   - ```python</span><br><span class="line">     class ScaledDotProductAttention(nn.Module):</span><br><span class="line">         &#x27;&#x27;&#x27; Scaled Dot-Product Attention &#x27;&#x27;&#x27;</span><br><span class="line">     </span><br><span class="line">         def __init__(self, temperature, attn_dropout=0.1):</span><br><span class="line">             super().__init__()</span><br><span class="line">             self.temperature = temperature</span><br><span class="line">             self.dropout = nn.Dropout(attn_dropout)</span><br><span class="line">     </span><br><span class="line">         def forward(self, q, k, v, mask=None):</span><br><span class="line">             # self.temperature是论文中的d_k ** 0.5，防止梯度过大</span><br><span class="line">             # QxK/sqrt(dk)</span><br><span class="line">             attn = torch.matmul(q / self.temperature, k.transpose(2, 3))</span><br><span class="line">     </span><br><span class="line">             if mask is not None:</span><br><span class="line">                 # 屏蔽不想要的输出</span><br><span class="line">                 attn = attn.masked_fill(mask == 0, -1e9)</span><br><span class="line">             # softmax+dropout</span><br><span class="line">             attn = self.dropout(F.softmax(attn, dim=-1))</span><br><span class="line">             # 概率分布xV</span><br><span class="line">             output = torch.matmul(attn, v)</span><br><span class="line">     </span><br><span class="line">             return output, attn</span><br><span class="line">         </span><br><span class="line">     class MultiHeadAttention(nn.Module):</span><br><span class="line">         &#x27;&#x27;&#x27; Multi-Head Attention module &#x27;&#x27;&#x27;</span><br><span class="line">     </span><br><span class="line">         # n_head头的个数，默认是8</span><br><span class="line">         # d_model编码向量长度，例如本文说的512</span><br><span class="line">         # d_k, d_v的值一般会设置为 n_head * d_k=d_model，</span><br><span class="line">         # 此时concat后正好和原始输入一样，当然不相同也可以，因为后面有fc层</span><br><span class="line">         # 相当于将可学习矩阵分成独立的n_head份</span><br><span class="line">         def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):</span><br><span class="line">             super().__init__()</span><br><span class="line">             # 假设n_head=8，d_k=64</span><br><span class="line">             self.n_head = n_head</span><br><span class="line">             self.d_k = d_k</span><br><span class="line">             self.d_v = d_v</span><br><span class="line">             # d_model输入向量，n_head * d_k输出向量</span><br><span class="line">             # 可学习W^Q，W^K,W^V矩阵参数初始化</span><br><span class="line">             self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)</span><br><span class="line">             self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)</span><br><span class="line">             self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)</span><br><span class="line">             # 最后的输出维度变换操作</span><br><span class="line">             self.fc = nn.Linear(n_head * d_v, d_model, bias=False)</span><br><span class="line">             # 单头自注意力</span><br><span class="line">             self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)</span><br><span class="line">             self.dropout = nn.Dropout(dropout)</span><br><span class="line">             # 层归一化</span><br><span class="line">             self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)</span><br><span class="line">     </span><br><span class="line">         def forward(self, q, k, v, mask=None):</span><br><span class="line">             # 假设qkv输入是(b,100,512),100是训练每个样本最大单词个数</span><br><span class="line">             # 一般qkv相等，即自注意力</span><br><span class="line">             residual = q</span><br><span class="line">             # 将输入x和可学习矩阵相乘，得到(b,100,512)输出</span><br><span class="line">             # 其中512的含义其实是8x64，8个head，每个head的可学习矩阵为64维度</span><br><span class="line">             # q的输出是(b,100,8,64),kv也是一样</span><br><span class="line">             q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)</span><br><span class="line">             k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)</span><br><span class="line">             v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)</span><br><span class="line">     </span><br><span class="line">             # 变成(b,8,100,64)，方便后面计算，也就是8个头单独计算</span><br><span class="line">             q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)</span><br><span class="line">     </span><br><span class="line">             if mask is not None:</span><br><span class="line">                 mask = mask.unsqueeze(1)   # For head axis broadcasting.</span><br><span class="line">             # 输出q是(b,8,100,64),维持不变,内部计算流程是：</span><br><span class="line">             # q*k转置，除以d_k ** 0.5，输出维度是b,8,100,100即单词和单词直接的相似性</span><br><span class="line">             # 对最后一个维度进行softmax操作得到b,8,100,100</span><br><span class="line">             # 最后乘上V，得到b,8,100,64输出</span><br><span class="line">             q, attn = self.attention(q, k, v, mask=mask)</span><br><span class="line">     </span><br><span class="line">             # b,100,8,64--&gt;b,100,512</span><br><span class="line">             q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)</span><br><span class="line">             q = self.dropout(self.fc(q))</span><br><span class="line">             # 残差计算</span><br><span class="line">             q += residual</span><br><span class="line">             # 层归一化，在512维度计算均值和方差，进行层归一化</span><br><span class="line">             q = self.layer_norm(q)</span><br><span class="line">     </span><br><span class="line">             return q, attn</span><br></pre></td></tr></table></figure></li></ul><h3><span id="feedforward-network"><strong>feedforward network</strong></span></h3><ul><li><pre><code class="lang-python">class PositionwiseFeedForward(nn.Module):    &#39;&#39;&#39; A two-feed-forward-layer module &#39;&#39;&#39;    def __init__(self, d_in, d_hid, dropout=0.1):        super().__init__()        # 两个fc层，对最后的512维度进行变换        self.w_1 = nn.Linear(d_in, d_hid) # position-wise        self.w_2 = nn.Linear(d_hid, d_in) # position-wise        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)        self.dropout = nn.Dropout(dropout)    def forward(self, x):        residual = x        x = self.w_2(F.relu(self.w_1(x)))        x = self.dropout(x)        x += residual        x = self.layer_norm(x)        return x</code></pre></li></ul><h3><span id="repeat-step-3"><strong>Repeat step 3</strong></span></h3><ul><li><script type="math/tex; mode=display">X_{hidden} = X_{attention} + X_{hidden} \\X_{hidden} = LayerNorm(X_{hidden}) \\ X_{hidden} \in R^{batch size * seq. len.* embed.dim. }</script></li><li></li></ul><h2><span id="0x02-pre-train">0x02 Pre train</span></h2><blockquote><ul><li><strong>PTM:</strong> Pre-train Model, 预训练模型</li><li><strong>LM</strong>: Language Model，语言模型</li><li><strong>AR</strong>: Auto-Regressive，自回归</li><li><strong>AE</strong>: Auto-Encoding，自编码</li><li><strong>CLM</strong>: Causual Language Model</li><li><strong>MLM</strong>: Masked Language Model</li><li><strong>PLM</strong>: Permuted Language Model</li><li><strong>NLU</strong>: Natural Language Understanding</li><li><strong>NLG</strong>: Natural Language Generation</li></ul></blockquote><h3><span id="1-whats-the-pre-train">1 What’s the pre train</span></h3><ul><li>Model parameters are no longer randomly initialized but are pretrained by some task such as language model.</li><li>The training task is decomposed into two steps: common learning and feature learning.</li></ul><h3><span id="2-how-to-pre-train">2 How to pre train</span></h3><h4><span id="21-two-basic-paradigms-autoregressivear-pretraining-and-autoencoderae-pretaining">2.1 Two basic paradigms: autoregressive(AR) pretraining and autoencoder(AE) pretaining</span></h4><p>GPT and BERT represent the two most basic pre-training paradigms. They are called “autoregressive”(e.g., GPT) and “autoencoder pre-training” (e.g., BERT), and they are suitable for different types of downstream tasks, with <strong>GPT</strong> often more suitable for <strong>text generation tasks</strong> and <strong>BERT</strong> often more suitable for <strong>text understanding tasks</strong>. Both are based on partial parameters of the Transformer architecture.</p><ul><li>GPT corresponds to the pretraining of the <strong>decoder</strong><ul><li>GPT’s full name is Generative Pre-Training.</li></ul></li><li>BERT corresponds to the pretraining of the <strong>encoder</strong><ul><li>BERT’s full name is <strong>Bidirectional</strong> Encoder Representations form Transformers.</li></ul></li></ul><h4><span id="211-gpt-gt-arlm-applicable-to-the-nlg-task">2.1.1 GPT —&gt; AR/LM , applicable to the NLG task</span></h4><p>The optimization goal of GPT is the joint probability of the model sequence (from left to right or from right to left), which is a traditional language model, and the latter predicted word is conditioned on the first predicted word, which is suitable for text generation tasks.</p><script type="math/tex; mode=display">p(x_{1;T}) = \Pi_{t=1}^Tp(x_t|x_{0:t-1})</script><h4><span id="212-bert-gt-aemlm-applicable-to-the-nlu-task">2.1.2 BERT —&gt; AE/MLM, applicable to the NLU task</span></h4><p>BERT works in a bidirectional way by replacing some tokens with special [MASK] characters and predicting these replaced characters on the target side.</p><h3><span id="3-t5-model-nlp-text-to-text">3 T5 model: NLP Text-to-Text</span></h3><ul><li>Full name is Transfer Text-to-Text Transformer.</li><li>Turn all NLP tasks into Text-to-Text tasks.</li><li>Data cleaning process<ul><li>Only lines ending with a normal symbol are kept.</li><li>Delete any pages containing Bad Words, specific words refer to <strong><a href="https://link.zhihu.com/?target=https%3A//github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words">List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words</a></strong></li><li>Lines that contain Javascript words are removed.</li><li>Pages containing curly braces commonly used in programming languages.</li><li>Any page that contains “lorem ipsum”</li><li>Repeat the situation in three consecutive sentences and keep one.</li></ul></li></ul><h4><span id="architecture-the-best-one">Architecture: The Best One</span></h4><p><img src="https://pic2.zhimg.com/80/v2-b1a8d9af6110e6d1b6a7615fc300a229_720w.webp" alt="img"></p><p>T5 model actually is the Encoder-Decoder model of Transformer.</p><h4><span id="objectives-search-search-search">Objectives: Search, Search, Search</span></h4><p><img src="https://pic3.zhimg.com/80/v2-247e53593f78282caf557d84c1d2c1fa_720w.webp" alt="img"></p><ol><li><p>First Part: Comparison of high-level methods (self-supervised pre-training methods)</p><ol><li>Language model style, from left to right;</li><li>Bert-style, destroy a part, and then restore it;</li><li>Deshuffling is to shuffle the text and then restore it.</li></ol><ul><li>Bert-style is the best!</li></ul></li><li><p>Second Part: A strategy when a section of text is corrupted.</p><ol><li>Mask, as most models do today, replace the corrupted token with a special character such as [M];</li><li>The replace span method, it can be regarded as synthesizing a special character from adjacent [M] in the Mask method above, and replacing a special character for each small segment to improve the calculation efficiency.</li><li>Drop method, without replacement operation, directly drops some characters at random.</li></ol><ul><li>Replace spans is the best!</li></ul></li><li><p>Third Part: What percentage of the text should be corrupted</p><ul><li>15% is the best!</li></ul></li><li><p>Forth Part: Destroy a small segment of approximately how long</p><ul><li>3 is the best!</li></ul></li></ol><h3><span id="datasets">Datasets</span></h3><h2><span id="0x04-ganlm">0x04 GANLM</span></h2><hr><p>版权信息</p>]]></content:encoded>
      
      
      
      
      <comments>http://www.warmfire.com/2023/11/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>数分第13周学习日志</title>
      <link>http://www.warmfire.com/2023/11/25/%E6%95%B0%E5%88%86%E7%AC%AC13%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97-DESKTOP-P9VTQEA/</link>
      <guid>http://www.warmfire.com/2023/11/25/%E6%95%B0%E5%88%86%E7%AC%AC13%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97-DESKTOP-P9VTQEA/</guid>
      <pubDate>Sat, 25 Nov 2023 00:17:29 GMT</pubDate>
      
      <description>&lt;p&gt;0x00 前言&lt;/p&gt;
&lt;p&gt;不定积分&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>0x00 前言</p><p>不定积分</p><span id="more"></span><h2><span id="0x01-不定积分定义与基本性质">0x01 不定积分定义与基本性质</span></h2><h3><span id="不定积分的定义">不定积分的定义</span></h3><ul><li>定义：如果存在函数 $F(x),F’(x)=f(x),\forall x\in I,$则称 $F(x)$为 $f(x)$在集合I上原函数</li><li>定义：函数 $f(x)$在集合I上所有原函数，称为 $f(x)$在集合I上的不定积分。</li><li>$\int{f(x)dx}=F(x)+c$<ul><li>$\int$ 积分符号</li><li>$x$ 积分变量</li><li>$f(x)$ 被积函数</li><li>$f(x)dx$被积表达式</li></ul></li></ul><h3><span id="基本性质">基本性质</span></h3><ol><li>若函数 $f(x)$的原函数存在，则 $kf(x)$存在原函数，且有 $\int kfdx = k\int fdx, \forall k\in R$</li><li>若函数 $f(x)$和 $g(x)$的原函数存在，则 $f(x)\pm g(x)$存在原函数，且 $\int(f\pm g)dx=\int fdx+\int gdx$</li><li>$\frac{d}{dx}[\int f(x)dx]=f(x); d[\int f(x)dx]=f(x)dx;\int dF(x)=F(x)+c;$</li><li>微分运算与积分运算互逆</li></ol><h3><span id="积分表">积分表</span></h3><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231125100925999.png" alt="image-20231125100925999"></p><p>要注意 $\ln|x| $</p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231125100939645.png" alt="image-20231125100939645"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231125100953784.png" alt="image-20231125100953784"></p><p>含有第一类间断点的函数没有原函数。</p><h2><span id="0x02-第一类换元公式及应用">0x02 第一类换元公式及应用</span></h2><h3><span id="换元公式">换元公式</span></h3><p>若 $f(u)$在区间I上有原函数 $F(u),\phi(x)$在J上可导，$\{u|u=\phi(x),\forall x\in J\}$，则 $F(\phi(x))$是 $f(\phi(x))\phi’(x)$在区间J上的原函数，即有：$\int f(\phi(x))\phi’(x)dx=\int f(u)du = F(u)+C = F(\phi(x))+C$</p><h3><span id="应用">应用</span></h3><h4><span id="积分表">积分表</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231125113508778.png" alt="image-20231125113508778"></p><h2><span id="0x03-分部积分公式及应用">0x03 分部积分公式及应用</span></h2><ul><li>$(uv)’=u’v+uv’, uv’=(uv)’-u’v$</li><li>$\int u(x)v’(x)dx = u(x)v(x)-\int u’(x)v(x)dx$</li><li>$\int vdu$ easier than $\int udv$</li><li>有递推关系的可以设为 $I_n$</li><li>三角函数相关的部分很有可能有周期性</li></ul><h2><span id="0x04-第二类换元公式及应用">0x04 第二类换元公式及应用</span></h2><h3><span id="定义">定义</span></h3><ul><li>设 $x=\psi(t)$是在区间上J单调可导的函数，并且 $\psi’(t)\neq 0$，又 $f(\psi(t))\psi’(t)$在区间J上存在原函数，则在J上有换元公式 $\int f(x)dx = \int f(\psi(t))\psi’(t)dt|_{t=\psi^{-1}(x)}$，其中 $t=\psi^{-1}(x)$为 $x=\psi(t)$的反函数</li><li>第一类换元是将多项凑成一项合并，定义域没有变化，第二类换元是将被积函数进行变量替换分解，注意其自身定义域的变化</li><li>一定要根据定义域进行讨论</li><li>常用三角代换的方式，目的是化解其中的根式</li></ul><h3><span id="积分表">积分表</span></h3><ul><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231125145048519.png" alt="image-20231125145048519"></li></ul><p>当分母阶较高时，可以采用倒代换</p><h2><span id="0x05-有理函数及有理三角函数的不定积分">0x05 有理函数及有理三角函数的不定积分</span></h2><h3><span id="有理函数">有理函数</span></h3><ul><li>定义：形如 $R(x)=\frac{P(x)}{Q(x)}$的函数，称为有理函数，其中 $P(x),Q(x)$分别是多项式。</li><li>若 $P(x)$的次数大于 $Q(x)$的次数，R(x)为假分式，类比于假分数</li></ul><h3><span id="有理函数分解定理">有理函数分解定理</span></h3><p>设 $R(x)=\frac{P(x)}{Q(x)}$为一个真分式，分母有分解式 $Q(x)=(x-a)^{n_1}\cdots(x-b)^{n_k}(x^2+px+q)^{m_1}\cdots(x^2+rx+s)^{m_i}$，所有二次式没有实根。</p><ul><li>注1：$f(x)=(x-a)^kg(x),g(a)\neq 0$，则 $x=a$为f(x)的k重根</li><li>注2：$x=a$为k重根 $\Leftrightarrow f^{(i)}(a)=0,i=0,1\cdots,k-1, f^{(k)}(a)\neq 0$</li></ul><h3><span id="三角函数有理式">三角函数有理式</span></h3><p>利用万能公式转换为有理函数的形式</p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231125173828993.png" alt="image-20231125173828993"></p><h3><span id="无理根式的不定积分">无理根式的不定积分</span></h3><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231125174213069.png" alt="image-20231125174213069"></p><p><hr><br>版权信息</p>]]></content:encoded>
      
      
      
      
      <comments>http://www.warmfire.com/2023/11/25/%E6%95%B0%E5%88%86%E7%AC%AC13%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97-DESKTOP-P9VTQEA/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>数分第13周学习日志</title>
      <link>http://www.warmfire.com/2023/11/25/%E6%95%B0%E5%88%86%E7%AC%AC13%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</link>
      <guid>http://www.warmfire.com/2023/11/25/%E6%95%B0%E5%88%86%E7%AC%AC13%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</guid>
      <pubDate>Sat, 25 Nov 2023 00:17:29 GMT</pubDate>
      
      <description>&lt;p&gt;0x00 前言&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>0x00 前言</p><span id="more"></span><h2><span id="0x01">0x01</span></h2><h2><span id="0x02">0x02</span></h2><h2><span id="0x03">0x03</span></h2><h2><span id="0x04">0x04</span></h2><p><hr><br>版权信息</p>]]></content:encoded>
      
      
      
      
      <comments>http://www.warmfire.com/2023/11/25/%E6%95%B0%E5%88%86%E7%AC%AC13%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>CE Log</title>
      <link>http://www.warmfire.com/2023/11/22/CE-Log/</link>
      <guid>http://www.warmfire.com/2023/11/22/CE-Log/</guid>
      <pubDate>Wed, 22 Nov 2023 13:45:14 GMT</pubDate>
      
      <description>&lt;p&gt;0x00 前言&lt;/p&gt;
&lt;p&gt;记录学习内容&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>0x00 前言</p><p>记录学习内容</p><span id="more"></span><h2><span id="0x01-教程部分">0x01  教程部分</span></h2><h3><span id="步骤9注入">步骤9：注入++</span></h3><p>在分析结构的过程中要想明白，比如我们查找的血量，很有可能是一个结构体的实例的一个属性值，因此内存空间上其他属性的分布极有可能是连续的，因此直接分析改变值对应的地址很可能猜出来其结构</p><h2><span id="0x02">0x02</span></h2><h2><span id="0x03">0x03</span></h2><h2><span id="0x04">0x04</span></h2><p><hr><br>版权信息</p>]]></content:encoded>
      
      
      
      
      <comments>http://www.warmfire.com/2023/11/22/CE-Log/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>可证明安全第12周学习日志</title>
      <link>http://www.warmfire.com/2023/11/21/%E5%8F%AF%E8%AF%81%E6%98%8E%E5%AE%89%E5%85%A8%E7%AC%AC12%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</link>
      <guid>http://www.warmfire.com/2023/11/21/%E5%8F%AF%E8%AF%81%E6%98%8E%E5%AE%89%E5%85%A8%E7%AC%AC12%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</guid>
      <pubDate>Tue, 21 Nov 2023 00:58:47 GMT</pubDate>
      
      <description>&lt;p&gt;0x00 前言&lt;/p&gt;
&lt;p&gt;Message Authentication Code&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>0x00 前言</p><p>Message Authentication Code</p><span id="more"></span><h2><span id="0x01-definition-and-security-of-mac">0x01  Definition and Security of MAC</span></h2><h3><span id="integrity">Integrity</span></h3><ul><li>ensuring that a received message originated from the intended party, and was not modified</li><li>even if an attacker controls the channel</li><li>use message authentication code</li></ul><h3><span id="definition-of-mac">Definition of MAC</span></h3><p>A message authentication code is defined by three PPT algorithms (Gen, Mac, Vrfy):</p><ul><li>Gen: takes as input $1^n$; outputs $k$. (Assume $|k|\geq n$)</li><li>Mac: takes as input key $k$ and message $m\in\{0,1\}^*$; outputs tag $t$, $t:= Mac_k(m)$</li><li>Vrfy: takes key $k$, message $m$, and tag $t$ as input; outputs 1 or 0</li></ul><h3><span id="security">Security</span></h3><h4><span id="threat-model">Threat model</span></h4><ul><li>adaptive chosen-message attack</li><li>assume the attacker can induce the sender to authenticate messages of the attacker’s choice</li></ul><h4><span id="security-goal">Security goal</span></h4><ul><li>existential unforgeability</li><li>attacker should be unable to forge a valid tag on any message not previously authenticated by the sender</li></ul><h3><span id="formal-definition">Formal Definition</span></h3><h4><span id="def-42-security-definition-of-mac">Def. 4.2 Security Definition of Mac</span></h4><ul><li>Fix $\mathcal{A},\Pi$</li><li>Define randomized experiment $Forge_{\mathcal{A},\Pi}(n)$:<ol><li>$k \leftarrow Gen(1^n)$</li><li>$\mathcal{A}$ interacts with an oracle $Mac_k(\cdot)$; let $\mathcal{Q}$ be the set of messages submitted to this oracle</li><li>$(m,t)\leftarrow \mathcal{A}$</li><li>$\mathcal{A}$ succeeds, and the experiment evaluates to 1,  if $Vrfy_k(m,t)=1$ and $m \not\in \mathcal{Q}$</li></ol></li><li>A message authentication code $\Pi = (Gen, Mac, Vrfy)$ is existentially unforgeable under an adaptive chosen-message attack if for all PPT attackers $\mathcal{A}$, there is a negligible function $\epsilon$ that $\Pr[Mac-Forge_{\mathcal{A},\Pi}=1]\leq \epsilon(n)$</li></ul><h3><span id="replay-attacks">Replay Attacks</span></h3><ul><li>no stateless mechanism can prevent them</li><li></li></ul><h2><span id="0x02-a-fixed-length-mac">0x02 A fixed-length MAC</span></h2><h3><span id="construction">Construction</span></h3><h4><span id="definition-46-secure-mac">Definition 4.6 Secure Mac</span></h4><ul><li>if F is a pseudomrandom function, then the above is a secure fixed-length MAC for message of length n.</li><li></li></ul><h2><span id="0x03">0x03</span></h2><h2><span id="0x04">0x04</span></h2><p><hr><br>版权信息</p>]]></content:encoded>
      
      
      
      
      <comments>http://www.warmfire.com/2023/11/21/%E5%8F%AF%E8%AF%81%E6%98%8E%E5%AE%89%E5%85%A8%E7%AC%AC12%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>数分第11周学习日志</title>
      <link>http://www.warmfire.com/2023/11/19/%E6%95%B0%E5%88%86%E7%AC%AC11%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</link>
      <guid>http://www.warmfire.com/2023/11/19/%E6%95%B0%E5%88%86%E7%AC%AC11%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</guid>
      <pubDate>Sun, 19 Nov 2023 06:34:01 GMT</pubDate>
      
      <description>&lt;p&gt;0x00 前言&lt;/p&gt;
&lt;p&gt;泰勒公式&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>0x00 前言</p><p>泰勒公式</p><span id="more"></span><h2><span id="0x01-微分的定义与计算">0x01  微分的定义与计算</span></h2><h3><span id="1-微分的定义">1 微分的定义</span></h3><h4><span id="定义">定义</span></h4><p>设函数 $y=f(x)$定义在 $U(x_0;\delta),x_0+\Delta x\in (x_0;\delta)$，如果成立：$\Delta y=f(x_0+\Delta x)-f(x_0)=A\cdot\Delta x+o(\Delta x)$，其中A是与 $\Delta x$无关的常熟，则称函数在点 $x_0$可微，$A\cdot\Delta x$称为函数在该点相应于自变量增量的微分，记作：$dy|_{x=x_0}=A\cdot\Delta x, or \ df(x_0)=A\cdot\Delta x$</p><h4><span id="等价定义">等价定义</span></h4><p>$\Delta y = f(x_0+\Delta x)-f(x_0)=A\cdot\Delta x+o(\Delta x)$</p><ul><li>$f(x)-(f(x_0)+A(x-x_0))=o(x-x_0)$</li></ul><h4><span id="可微条件">可微条件</span></h4><ul><li>定理：函数 $f(x)$在点 $x_0$可微的充要条件是函数 $f(x)$在点 $x_0$可导，且 $A=f’(x_0)$</li><li>注1：函数 $f(x)$在 $\forall x\in U(x_0;\delta)$ 可微，记为 $dy \ or \ df(x),i.t. dy=f’(x)\Delta x$</li><li>注2：自变量x的增量 $\Delta x$视为自变量的微分</li></ul><h4><span id="运算法则">运算法则</span></h4><p>与求导的法则相似</p><h4><span id="高阶微分">高阶微分</span></h4><p>$dx^n=(dx)^n,f^{(n)}(x)=\frac{d^nf(x)}{dx^n}$</p><h4><span id="微分形式的不变性">微分形式的不变性</span></h4><p>复合函数二阶微分不变，高阶变</p><h2><span id="0x02-微分的计算">0x02 微分的计算</span></h2><h3><span id="1-基本初等函数的微分公式">1 基本初等函数的微分公式</span></h3><p>与求导相似，带了一个 $dx$</p><h3><span id="应用近似公式">应用：近似公式</span></h3><h2><span id="0x03-带有peano余项的泰勒公式">0x03 带有Peano余项的泰勒公式</span></h2><h3><span id="1-带有peano余项的泰勒公式证明">1 带有Peano余项的泰勒公式证明</span></h3><h3><span id="2-带有peano余项的泰勒公式">2 带有Peano余项的泰勒公式</span></h3><p>$f(x)=p_n(x)+o[(x-x_0)^n]$ —- ①</p><p>$p_n(x)=\sum^n_{k=0}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k$</p><p>当 $x_0=0$，称为麦克劳林公式</p><h3><span id="3-常用函数展开">3 常用函数展开</span></h3><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231119222204635.png" alt="image-20231119222204635"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231119222218627.png" alt="image-20231119222218627"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231119222437511.png" alt="image-20231119222437511"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231119222501948.png" alt="image-20231119222501948"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231119222644899.png" alt="image-20231119222644899"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231119222659182.png" alt="image-20231119222659182"></p><h2><span id="0x04-带拉格朗日余项的泰勒公式">0x04 带拉格朗日余项的泰勒公式</span></h2><p>利用Cauchy中值定理进行证明余项原因</p><h3><span id="公式表示">公式表示</span></h3><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231119225543780.png" alt="image-20231119225543780"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231119225619413.png" alt="image-20231119225619413"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231119225626803.png" alt="image-20231119225626803"></p><h3><span id="常用函数">常用函数</span></h3><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231119225652984.png" alt="image-20231119225652984"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231119225716179.png" alt="image-20231119225716179"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231119225804737.png" alt="image-20231119225804737"></p><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231119225901342.png" alt="image-20231119225901342"></p><h3><span id="应用">应用</span></h3><h4><span id="1-极值问题">1 极值问题</span></h4><p><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231120085838499.png" alt="image-20231120085838499"></p><h4><span id="2-求函数极限">2 求函数极限</span></h4><p>用多项式替换复杂函数</p><h3><span id="典型例题">典型例题</span></h3><p>两种余项定理条件不同，一个n阶导，一个n+1阶导</p><p>Taylor公式可以将函数与其导函数联系到一起</p><p>多利用奇偶性的不同来实现除去特定项</p><h2><span id="提高课">提高课</span></h2><p><hr><br>版权信息</p>]]></content:encoded>
      
      
      
      
      <comments>http://www.warmfire.com/2023/11/19/%E6%95%B0%E5%88%86%E7%AC%AC11%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>数据库第12周学习日志</title>
      <link>http://www.warmfire.com/2023/11/17/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%AC12%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</link>
      <guid>http://www.warmfire.com/2023/11/17/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%AC12%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</guid>
      <pubDate>Fri, 17 Nov 2023 03:28:32 GMT</pubDate>
      
      <description>&lt;p&gt;0x00 前言&lt;/p&gt;
&lt;p&gt;关系数据库理论&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>0x00 前言</p><p>关系数据库理论</p><span id="more"></span><h2><span id="0x01-问题的提出">0x01  问题的提出</span></h2><ul><li><p>把关系模式看作一个三元组 $R<u,f>$</u,f></p></li><li><p>作为一个二维表，关系要符合一个最基本的条件：</p><ul><li><p>每个分量必须是不可分的数据项</p></li><li><p>满足了这个条件的关系模式属于<strong>第一范式（1NF）</strong></p></li></ul></li><li><p>数据依赖</p><ul><li>一个关系内部属性之间的一种约束关系</li><li>数据内在的性质，语义的体现</li><li>设计关系模式时，除给出属性全集外，还需给出数据依赖集合</li><li>分为两类：函数依赖（FD）、多值依赖（MVD）</li></ul></li><li><p>$R(U,F)$中存在的问题</p><ul><li>数据冗余</li><li>更新异常</li><li>插入异常</li><li>删除异常</li><li>原因：模式中某些数据依赖引起的</li><li>解决：用规范化理论改造关系模式消除其中不合适的数据依赖</li><li></li></ul></li></ul><h2><span id="0x02-规范化">0x02 规范化</span></h2><h3><span id="1-函数依赖">1 函数依赖</span></h3><h4><span id="函数依赖">函数依赖</span></h4><ul><li>定义：设 $R(U)$时一个属性集U上的关系模式，$X,Y$是U的子集。若对于 $R(U)$的<strong>任意一个可能关系r</strong>，r中不可能<strong>存在两个元组</strong>在X上的属性值相等，而在Y上的属性值不等，则称 <code>X函数确定Y</code>或 <code>Y函数依赖于X</code>，记作 $X\rightarrow Y$</li><li>$X\rightarrow Y,Y\rightarrow X \Longrightarrow X \leftarrow\rightarrow Y$</li><li>若Y不函数依赖于X，则记为 $X\not\rightarrow Y$</li><li>函数依赖是R的所有关系实例均要满足的约束条件</li><li>语义概念，只能根据数据的语义来确定一个函数依赖</li><li>本质上是对属性间取值的一种约束，是一种数据依赖，是问题域业务规则的体现</li></ul><h4><span id="平凡函数依赖与非平凡函数依赖">平凡函数依赖与非平凡函数依赖</span></h4><ul><li>$X\rightarrow Y , but\ Y \not\subseteq X,then \ X\rightarrow Y$是非平凡的函数依赖</li><li>$X\rightarrow Y , and\ Y \subseteq X,then \ X\rightarrow Y$是平凡的函数依赖</li><li>平凡函数依赖必然成立</li><li>$X\rightarrow Y$，则称X为这个函数依赖的决定因素</li></ul><h4><span id="完全函数依赖与部分函数依赖">完全函数依赖与部分函数依赖</span></h4><ul><li>在 $R(U)$中，如果 $X\rightarrow Y$，并且对于X的任何一个真子集 $X’$，都有 $X’\not\rightarrow Y$，则称Y对X完全函数依赖，记作 $X\overset{F}{\rightarrow}Y$</li><li>若 $X\rightarrow Y$​，但Y不完全函数依赖于X，则称Y对X部分函数依赖，记作$X\overset{P}{\rightarrow}Y$</li></ul><h4><span id="传递函数依赖">传递函数依赖</span></h4><ul><li>定义：在 $R(U)$中，如果 $X\rightarrow Y , Y \not\subseteq X,Y\not{\rightarrow}X, Y\rightarrow Z , Z \not\subseteq Y$，则称Z对X传递函数依赖，记为 $X\overset{传递}{\rightarrow}Z$</li><li>如果 $Y\rightarrow X$则Z直接依赖于X，而不是传递依赖</li><li>存在非受控冗余</li></ul><h3><span id="2-码键">2 码/键</span></h3><ul><li>定义：设K为 $R<u,f>$中的属性或属性组合，若 $K\overset{F}{\rightarrow}U$，则K称为R的一个候选键/码</u,f></li><li>部分依赖则成为超键</li><li>候选键是最小的超码</li><li>多个候选码选定一个为主码</li><li>定义：关系模式R中属性或属性组X并非R的码，但X是另一个关系模式的码，则X是R的外部码</li><li>定义：对于满足一组函数依赖F的关系模式 $R<u,f>$，其中任何一个关系r，若函数依赖 $X\rightarrow Y$成立，则称F逻辑蕴涵 $X\rightarrow Y$</u,f></li><li>Armstrong公理系统<ul><li>设<em>U</em>为属性集总体，<em>F</em>是<em>U</em>上的一组函数依赖， 于是有关系模式<em>R</em> &lt;<em>U</em>,<em>F</em> &gt;。对<em>R</em> &lt;<em>U</em>,<em>F</em>&gt; 来说有以下的推理规则：</li><li>A1 自反律：若 $Y\subseteq X\subseteq U$，则F逻辑蕴涵 $X\rightarrow Y$</li><li>A2 增广律：若 F逻辑蕴涵 $X\rightarrow Y$，且 $Z\subseteq U$，则F逻辑蕴涵 $XZ\rightarrow YZ$</li><li>A3 传递律：若F逻辑蕴涵 $X\rightarrow Y$与$Y\rightarrow Z$，则F逻辑蕴涵 $X\rightarrow Z$</li><li>三条推理规则：<ul><li>合并规则：由<em>X</em>→<em>Y</em>，<em>X</em>→<em>Z</em>，有<em>X</em>→<em>YZ</em>。</li><li>伪传递规则：由<em>X</em>→<em>Y</em>，<em>WY</em>→<em>Z</em>，有<em>XW</em>→<em>Z</em>。</li><li>分解规则：$X\rightarrow Y, Z\subseteq Y$,有 $X\rightarrow Z$</li></ul></li></ul></li></ul><h3><span id="3-范式-nf">3 范式 NF</span></h3><ul><li>符合某一种级别的关系模式的集合</li><li>种类<ul><li>第一范式(1NF)</li><li>第二范式(2NF)</li><li>第三范式(3NF)</li><li>BC范式(BCNF)</li><li>第四范式(4NF)</li><li>第五范式(5NF)</li></ul></li><li><img src="http://warmfire-store.oss-cn-beijing.aliyuncs.com//img/image-20231117152439523.png" alt="image-20231117152439523"></li><li>一个低一级范式的关系模式，通过模式分解，可以转换为若干个高一级范式的关系模式的集合，这个过程称为规范化</li></ul><h3><span id="4-2nf">4 2NF</span></h3><ul><li>定义：若关系模式 $R\in 1NF$，且每个非主属性都完全函数依赖于任何一个候选码，则 $R\in2NF$</li><li>关系模式不属于2NF，会产生以下问题：<ul><li>插入异常</li><li>删除异常</li><li>修改复杂</li></ul></li><li>解决的是非主属性对复合主键的部份依赖</li><li>单一候选键一定符合</li></ul><h3><span id="5-3nf">5 3NF</span></h3><ul><li>定义：设关系模式 $R<u,f>\in 1NF$，若R中不存在这样的码X、属性组Y及非主属性Z $(Z\not\subseteq Y),s.t. \ X\rightarrow Y , Y\rightarrow Z$成立，$Y\not\rightarrow X$，则称 $R<u,f>\in 3NF$</u,f></u,f></li><li>即不存在传递依赖</li><li>3NF通常解决的是非主属性之间的依赖关系，非主属性对候选键的传递依赖。</li><li>属性组Y可能包括部分主属性、非主属性、或二者组合</li></ul><h3><span id="6-bcnf">6 BCNF</span></h3><ul><li>设关系模式 $R<u,f>\in1NF$，若 $X\rightarrow Y \ and \ Y\not\subseteq X$时X必含有码，则 $R<u,f>\in BCNF$</u,f></u,f></li><li>每一个决定属性集都包含候选码</li><li>解决了主属性组（但不构成候选键）对候选键的部分或者传递依赖关系</li><li>即除了所有属性（组）对候选键的依赖关系之外，没有任何其他的依赖关系</li><li>总结：<ul><li>2NF解决非主属性对候选键的部分依赖</li><li>3NF解决非主属性对候选键的传递依赖</li><li>BCNF解决主属性对候选键的部分或传递依赖</li><li></li></ul></li></ul><h3><span id="7-多值依赖">7 多值依赖</span></h3><ul><li>定义：MVD 设 $R(U)$是属性集U上的一个关系模式。<em>X，Y，Z</em>是U的子集，且 $Z=U-X-Y$。关系模式<em>R(U)</em>中多值依赖 $X\rightarrow\rightarrow Y$成立，当且仅当对<em>R(U)</em>的任意关系r，给定的一对 $(x,z)$值，有一组Y的值，仅仅决定于x值而与z值无关</li><li>Y和Z相互独立</li><li>平凡多值依赖：Z为空的情况</li><li>性质：<ul><li>对称性：即若X→→Y，则X→→Z，其中Z＝U－X－Y</li><li>传递性：即若X→→Y，Y→→Z， 则X→→Z -Y。</li><li>函数依赖是多值依赖的特例：即若X→Y，则 X→→Y。</li><li>若X→→Y，X→→Z，则X→→YZ。</li><li>若X→→Y，X→→Z，则X→→Y∩Z。</li><li>若X→→Y，X→→Z，则X→→Y-Z，X→→Z -Y。</li></ul></li></ul><h3><span id="8-4nf">8 4NF</span></h3><ul><li>定义：设关系模式 $R<u,f>\in1NF$，若 对于R的每个非平凡多值依赖 $X\rightarrow\rightarrow Y$ 时X都含有码，则 $R<u,f>\in 4NF$</u,f></u,f></li><li>性质：<ul><li>不允许有非平凡且非函数依赖的多值依赖</li><li>允许的非平凡多值依赖实际是函数依赖</li><li>平凡的多值依赖属于第四范式</li><li>4NF $\rightarrow$ BCNF</li></ul></li></ul><h2><span id="0x03-数据依赖的公理系统">0x03 数据依赖的公理系统</span></h2><h2><span id="0x04">0x04</span></h2><p><hr><br>版权信息</p>]]></content:encoded>
      
      
      
      
      <comments>http://www.warmfire.com/2023/11/17/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%AC12%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>数据库第11周学习日志</title>
      <link>http://www.warmfire.com/2023/11/16/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%AC11%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</link>
      <guid>http://www.warmfire.com/2023/11/16/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%AC11%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</guid>
      <pubDate>Thu, 16 Nov 2023 12:55:32 GMT</pubDate>
      
      <description>&lt;p&gt;0x00 前言&lt;/p&gt;
&lt;p&gt;数据库完整性&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>0x00 前言</p><p>数据库完整性</p><span id="more"></span><h2><span id="0x01-实体完整性">0x01  实体完整性</span></h2><h3><span id="1-实体完整性定义">1 实体完整性定义</span></h3><h4><span id="关系模型的实体完整性">关系模型的实体完整性</span></h4><ul><li>CREATE TABLE 用 PRIMARY KEY定义</li></ul><h4><span id="单属性构成的主键有两种说明方法">单属性构成的主键有两种说明方法</span></h4><ul><li>定义为列级约束条件</li><li>定义为表级约束条件</li></ul><h4><span id="对多个属性构成的主键只有一个说明方法">对多个属性构成的主键只有一个说明方法</span></h4><ul><li>定义为表级约束条件</li></ul><h3><span id="2-实体完整性检查和违约处理">2 实体完整性检查和违约处理</span></h3><p>插入或对主码列进行更新操作时，关系数据库管理系统按照实体完整性规则自动进行检查</p><ul><li>检查主码值是否唯一<ul><li>全表扫描：耗时</li><li>RDBMS在主码上自动建立一个索引</li></ul></li><li>检查主码的各个属性是否为空</li></ul><h2><span id="0x02-参照完整性">0x02 参照完整性</span></h2><h3><span id="1-参照完整性定义">1 参照完整性定义</span></h3><ul><li>用FOREIGN KEY定义外码</li><li>用REFERENCES指明外码参照哪些表的主码</li></ul><h3><span id="2-参照完整性检查和违约处理">2 参照完整性检查和违约处理</span></h3><ul><li><p>将两个表中的相应元组联系起来</p></li><li><p>两个表的增删改都有可能破坏参照完整性，必须进行检查</p></li><li></li><li><p>| <strong>被参照表（例如Student</strong>） | 参照表（例如SC）   | <strong>违约处理</strong>             |<br>| —————————————- | ————————— | ———————————— |<br>| 可能破坏参照完整性          | 插入元组           | 拒绝                     |<br>| 可能破坏参照完整性          | 修改外码值         | 拒绝                     |<br>| 删除元组                    | 可能破坏参照完整性 | 拒绝/级连删除/设置为空值 |<br>| 修改主码值                  | 可能破坏参照完整性 | 拒绝/级连修改/设置为空值 |</p></li></ul><ol><li>拒绝执行 NO ACTION：默认策略</li><li>级联操作 CASCADE：</li><li>设置为空值 SET NULL：</li></ol><ul><li>除了定义外码，还应该定义外码列是否允许空值</li></ul><h2><span id="0x03-用户定义的完整性">0x03 用户定义的完整性</span></h2><ul><li>针对某一具体应用的数据必须满足的语义要求</li></ul><h3><span id="1-属性上的约束条件">1 属性上的约束条件</span></h3><h4><span id="属性上约束条件的定义">属性上约束条件的定义</span></h4><ul><li>列值非空 NOT NULL</li><li>列值唯一 UNIQUE</li><li>检查列值是否满足一个条件表达式 CHECK</li></ul><h4><span id="属性上的约束条件检查和违约处理">属性上的约束条件检查和违约处理</span></h4><ul><li>约束条件RDBMS自动检查</li><li>不满足则拒绝执行</li></ul><h3><span id="2-元组上的约束条件">2 元组上的约束条件</span></h3><h4><span id="元组上约束条件的定义">元组上约束条件的定义</span></h4><ul><li>CHECK短语定义元组上的约束条件，元组级的限制</li><li>元组级限制可以设置不同属性之间的取值的相互约束条件</li></ul><h4><span id="元组上的约束条件检查和违约处理">元组上的约束条件检查和违约处理</span></h4><ul><li>约束条件RDBMS自动检查</li><li>不满足则拒绝执行</li></ul><h2><span id="0x04-完整性约束命名子句">0x04 完整性约束命名子句</span></h2><h3><span id="1-完整性约束命名子句">1 完整性约束命名子句</span></h3><p>CONSTRAINT &lt;完整性约束条件名&gt; &lt;完整性约束条件&gt;</p><h3><span id="2-修改表中的完整性限制">2 修改表中的完整性限制</span></h3><p>使用 ALTER TABLE语句修改表中的完整性限制</p><h2><span id="0x05-域中的完整性限制">0x05 域中的完整性限制</span></h2><h2><span id="0x06-断言">0x06 断言</span></h2><ul><li>SQL中，使用CREATE ASSERTION 语句，通过声明性断言来指定更具一般性的约束</li><li>可以定义涉及多个表的或聚集操作的比较复杂的完整性约束</li><li>任何对断言所涉及的关系的操作都会触发DDBMS的检查，断言不为真值的操作会被拒绝。</li></ul><h3><span id="1-创建断言的语句格式">1 创建断言的语句格式</span></h3><p>CREATE ASSERTION &lt;断言名&gt; <check 子句></check></p><h3><span id="2-删除断言的语句格式">2 删除断言的语句格式</span></h3><p>DROP ASSERTION &lt;断言名&gt;</p><h2><span id="0x07-触发器">0x07 触发器</span></h2><ul><li>触发器 Trigger 是用户定义在关系表上的一类由事件驱动的特殊过程</li><li>触发器保存在数据库服务器中</li><li>任何增删改操作均由服务器自动激活相应的触发器</li></ul><h3><span id="1-定义触发器">1 定义触发器</span></h3><p>CREATE TRIGGER <trigger_name></trigger_name></p><p>{BEFORE| AFTER} <trigger_event> ON <table_name></table_name></trigger_event></p><p>REFERENCES NEW|OLD ROW AS <variables></variables></p><p>FOR EACH {ROW| STATEMENT}</p><p>[WHEN <trigger_condition>] <trigger_action></trigger_action></trigger_condition></p><ul><li>表的拥有者才可以在表上创建触发器</li><li>触发器名<ul><li>可以包含模式名，也可以不包含模式名</li><li>同一模式下，触发器名必须唯一</li><li>触发器名和表名必须在统一模式下</li></ul></li><li>表名<ul><li>触发器只能定义在基本表上，不能定义在视图上</li></ul></li><li>触发事件<ul><li>INSERT\DELETE\UPDATE</li><li>UPDATE OF {COLUMN}来限制具体某一列</li></ul></li><li>触发器类型<ul><li>行级    FOR EACH ROW</li><li>语句级  FOR EACH STATEMENT </li></ul></li><li>触发条件：condition为真</li><li>触发行动体：如果为行级触发器，可以使用new或old</li></ul><h3><span id="2-激活触发器">2 激活触发器</span></h3><ul><li>由触发事件激活</li><li>可能定义多个触发器，顺序：<ol><li>执行BEFORE触发器</li><li>激活触发器的SQL语句</li><li>执行AFTER触发器</li></ol></li></ul><h3><span id="3-删除触发器">3 删除触发器</span></h3><ul><li>DROP TRIGGER <trigger_name> ON <table_name></table_name></trigger_name></li><li></li></ul><p><hr><br>版权信息</p>]]></content:encoded>
      
      
      
      
      <comments>http://www.warmfire.com/2023/11/16/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%AC11%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
